#!/usr/bin/env python3
"""
TESTE ESPECÃFICO: llama-3.1-8b-instant (Groq) para Acesso aos Logs NLL

MODELO ALVO: llama-3.1-8b-instant
OBJETIVO: Verificar se este modelo especÃ­fico suporta logprobs para calcular NLL
"""

import os
import json
import math
import asyncio
import traceback
from typing import Dict, Any, List
from dotenv import load_dotenv

load_dotenv()

class LlamaMInstantNLLTester:
    """Testador especÃ­fico para llama-3.1-8b-instant"""
    
    def __init__(self):
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        self.model_name = "llama-3.1-8b-instant"
        
        # Prompts de teste variados
        self.test_prompts = [
            "Explain artificial intelligence in simple terms.",
            "What is machine learning?",
            "Define neural networks briefly.",
            "How does deep learning work?"
        ]
        
        # Configurar cliente
        if self.groq_api_key:
            try:
                from groq import Groq
                self.client = Groq(api_key=self.groq_api_key)
                print(f"âœ… Cliente Groq configurado para {self.model_name}")
            except ImportError:
                print("âŒ Biblioteca Groq nÃ£o disponÃ­vel")
                self.client = None
        else:
            print("âŒ GROQ_API_KEY nÃ£o encontrada")
            self.client = None
    
    def print_section(self, title: str):
        """Imprimir seÃ§Ã£o formatada"""
        print(f"\n{'='*60}")
        print(f" {title}")
        print(f"{'='*60}")
    
    async def test_basic_functionality(self) -> Dict[str, Any]:
        """Teste bÃ¡sico de funcionalidade"""
        
        print("ğŸ” TESTE 1: Funcionalidade BÃ¡sica")
        print("-" * 40)
        
        if not self.client:
            return {"error": "Cliente nÃ£o disponÃ­vel"}
        
        try:
            prompt = self.test_prompts[0]
            print(f"ğŸ“ Prompt: {prompt}")
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=100
            )
            
            response_text = response.choices[0].message.content
            print(f"âœ… Resposta recebida ({len(response_text)} chars)")
            print(f"ğŸ“¤ Resposta: {response_text[:150]}...")
            
            return {
                "status": "success",
                "prompt": prompt,
                "response": response_text,
                "response_length": len(response_text)
            }
            
        except Exception as e:
            print(f"âŒ Erro na funcionalidade bÃ¡sica: {e}")
            return {"error": str(e)}
    
    async def test_logprobs_comprehensive(self) -> Dict[str, Any]:
        """Teste abrangente de logprobs"""
        
        print("\nğŸ” TESTE 2: Logprobs Detalhado")
        print("-" * 40)
        
        if not self.client:
            return {"error": "Cliente nÃ£o disponÃ­vel"}
        
        prompt = self.test_prompts[1]
        print(f"ğŸ“ Prompt: {prompt}")
        
        # Teste 1: Logprobs bÃ¡sico
        print("\nğŸ¯ Subteste 2A: Logprobs BÃ¡sico")
        try:
            response_basic = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=50,
                logprobs=True
            )
            
            print("âœ… Logprobs bÃ¡sico aceito!")
            
        except Exception as e:
            print(f"âŒ Logprobs bÃ¡sico rejeitado: {e}")
            return {"error": f"logprobs_basic_failed: {e}"}
        
        # Teste 2: Logprobs com top_logprobs
        print("\nğŸ¯ Subteste 2B: Logprobs com Top Alternatives")
        try:
            response_top = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=50,
                logprobs=True,
                top_logprobs=5
            )
            
            print("âœ… Top logprobs aceito!")
            
            # Analisar estrutura da resposta
            choice = response_top.choices[0]
            response_text = choice.message.content
            
            print(f"ğŸ“¤ Resposta: {response_text[:100]}...")
            
            result = {
                "status": "success",
                "model": self.model_name,
                "prompt": prompt,
                "response_text": response_text,
                "logprobs_available": False,
                "nll_calculable": False
            }
            
            # Verificar se logprobs estÃ£o presentes
            if hasattr(choice, 'logprobs') and choice.logprobs:
                print("ğŸ¯ ANALISANDO LOGPROBS:")
                logprobs = choice.logprobs
                
                print(f"   Tipo logprobs: {type(logprobs)}")
                
                # Verificar atributos
                attrs = [attr for attr in dir(logprobs) if not attr.startswith('_')]
                print(f"   Atributos: {attrs}")
                
                # Verificar content
                if hasattr(logprobs, 'content') and logprobs.content:
                    content_len = len(logprobs.content)
                    print(f"   ğŸ“Š Content tokens: {content_len}")
                    
                    if content_len > 0:
                        result["logprobs_available"] = True
                        
                        # Analisar primeiros tokens
                        print(f"\n   ğŸ” ANÃLISE DOS PRIMEIROS TOKENS:")
                        
                        total_logprob = 0.0
                        token_details = []
                        
                        for i, token_data in enumerate(logprobs.content[:5]):  # Primeiros 5 tokens
                            try:
                                token = getattr(token_data, 'token', 'N/A')
                                logprob = getattr(token_data, 'logprob', 0.0)
                                
                                print(f"      Token {i}: '{token}' -> logprob: {logprob:.4f}")
                                
                                total_logprob += logprob
                                token_details.append({
                                    "token": token,
                                    "logprob": logprob
                                })
                                
                                # Verificar top alternatives
                                if hasattr(token_data, 'top_logprobs') and token_data.top_logprobs:
                                    print(f"         Alternativas top:")
                                    for j, alt in enumerate(token_data.top_logprobs[:3]):
                                        alt_token = getattr(alt, 'token', 'N/A')
                                        alt_logprob = getattr(alt, 'logprob', 0.0)
                                        print(f"           {j+1}. '{alt_token}': {alt_logprob:.4f}")
                                
                            except Exception as token_error:
                                print(f"      âš ï¸ Erro no token {i}: {token_error}")
                        
                        # Calcular NLL se temos dados suficientes
                        if token_details and len(token_details) > 0:
                            nll = -total_logprob
                            perplexity = math.exp(nll / len(token_details))
                            
                            result.update({
                                "nll_calculable": True,
                                "nll_value": nll,
                                "perplexity": perplexity,
                                "total_logprob": total_logprob,
                                "token_count": len(logprobs.content),
                                "analyzed_tokens": len(token_details),
                                "token_details": token_details
                            })
                            
                            print(f"\n   ğŸ“Š MÃ‰TRICAS CALCULADAS:")
                            print(f"      âœ… NLL: {nll:.4f}")
                            print(f"      âœ… Perplexity: {perplexity:.4f}")
                            print(f"      âœ… Total tokens: {len(logprobs.content)}")
                            print(f"      âœ… Total logprob: {total_logprob:.4f}")
                        
                        else:
                            print("   âš ï¸ NÃ£o foi possÃ­vel extrair dados dos tokens")
                    
                    else:
                        print("   âŒ Content estÃ¡ vazio")
                
                else:
                    print("   âŒ Logprobs.content nÃ£o disponÃ­vel")
            
            else:
                print("âŒ Logprobs nÃ£o encontrados na resposta")
            
            return result
            
        except Exception as e:
            print(f"âŒ Top logprobs rejeitado: {e}")
            return {"error": f"top_logprobs_failed: {e}"}
    
    async def test_multiple_prompts(self) -> List[Dict[str, Any]]:
        """Testar mÃºltiplos prompts para verificar consistÃªncia"""
        
        print("\nğŸ” TESTE 3: MÃºltiplos Prompts")
        print("-" * 40)
        
        if not self.client:
            return [{"error": "Cliente nÃ£o disponÃ­vel"}]
        
        results = []
        
        for i, prompt in enumerate(self.test_prompts[2:], 3):  # Usar prompts restantes
            print(f"\nğŸ¯ Subteste 3.{i-2}: {prompt}")
            
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.2,
                    max_tokens=30,
                    logprobs=True,
                    top_logprobs=3
                )
                
                choice = response.choices[0]
                response_text = choice.message.content
                
                result = {
                    "prompt": prompt,
                    "response": response_text,
                    "has_logprobs": hasattr(choice, 'logprobs') and choice.logprobs is not None
                }
                
                if result["has_logprobs"]:
                    logprobs = choice.logprobs
                    if hasattr(logprobs, 'content') and logprobs.content:
                        # Calcular NLL rÃ¡pido
                        total_logprob = sum(
                            getattr(token, 'logprob', 0.0) 
                            for token in logprobs.content
                        )
                        nll = -total_logprob
                        
                        result.update({
                            "token_count": len(logprobs.content),
                            "nll_value": nll,
                            "logprobs_success": True
                        })
                        
                        print(f"   âœ… NLL: {nll:.4f} ({len(logprobs.content)} tokens)")
                    else:
                        result["logprobs_success"] = False
                        print(f"   âš ï¸ Logprobs sem content")
                else:
                    result["logprobs_success"] = False
                    print(f"   âŒ Sem logprobs")
                
                results.append(result)
                
            except Exception as e:
                print(f"   âŒ Erro: {e}")
                results.append({"prompt": prompt, "error": str(e)})
        
        return results
    
    def analyze_final_results(self, basic_result: Dict, logprobs_result: Dict, multiple_results: List[Dict]):
        """AnÃ¡lise final de todos os resultados"""
        
        self.print_section("ANÃLISE FINAL - llama-3.1-8b-instant")
        
        print(f"ğŸ¤– MODELO TESTADO: {self.model_name}")
        print(f"ğŸ”‘ API: Groq")
        
        # Status geral
        basic_ok = basic_result.get("status") == "success"
        logprobs_ok = logprobs_result.get("status") == "success"
        nll_ok = logprobs_result.get("nll_calculable", False)
        
        print(f"\nğŸ“Š RESUMO DOS TESTES:")
        print(f"   âœ… Funcionalidade bÃ¡sica: {'SIM' if basic_ok else 'NÃƒO'}")
        print(f"   ğŸ“Š Logprobs disponÃ­veis: {'SIM' if logprobs_ok else 'NÃƒO'}")
        print(f"   ğŸ¯ NLL calculÃ¡vel: {'SIM' if nll_ok else 'NÃƒO'}")
        
        if nll_ok:
            nll_value = logprobs_result.get("nll_value", 0)
            perplexity = logprobs_result.get("perplexity", 0)
            token_count = logprobs_result.get("token_count", 0)
            
            print(f"\nğŸ‰ SUCESSO - NLL CALCULÃVEL!")
            print(f"   ğŸ“ˆ NLL: {nll_value:.4f}")
            print(f"   ğŸ“ˆ Perplexity: {perplexity:.4f}")
            print(f"   ğŸ”¢ Tokens analisados: {token_count}")
            
            # Verificar consistÃªncia com mÃºltiplos prompts
            successful_multiples = [r for r in multiple_results if r.get("logprobs_success")]
            if successful_multiples:
                avg_nll = sum(r["nll_value"] for r in successful_multiples) / len(successful_multiples)
                print(f"   ğŸ“Š NLL mÃ©dio (mÃºltiplos prompts): {avg_nll:.4f}")
                print(f"   âœ… ConsistÃªncia: {len(successful_multiples)}/{len(multiple_results)} prompts")
        
        else:
            print(f"\nâŒ LIMITAÃ‡Ã•ES ENCONTRADAS:")
            
            if not basic_ok:
                print(f"   ğŸš« Funcionalidade bÃ¡sica falhou")
                print(f"   ğŸš« Erro: {basic_result.get('error', 'Desconhecido')}")
            
            elif not logprobs_ok:
                print(f"   ğŸš« Logprobs nÃ£o suportados")
                print(f"   ğŸš« Erro: {logprobs_result.get('error', 'Desconhecido')}")
            
            else:
                print(f"   ğŸš« Dados de logprobs insuficientes para calcular NLL")
        
        # ComparaÃ§Ã£o com outros modelos
        print(f"\nğŸ”„ COMPARAÃ‡ÃƒO COM OUTROS RESULTADOS:")
        print(f"   ğŸ”µ Google Gemini: âœ… NLL via sampling (~0.3-1.2)")
        print(f"   ğŸŸ¢ llama-3.1-8b-instant: {'âœ…' if nll_ok else 'âŒ'} NLL {'direto' if nll_ok else 'nÃ£o disponÃ­vel'}")
        
        # RecomendaÃ§Ã£o final
        print(f"\nğŸ’¡ RECOMENDAÃ‡ÃƒO PARA llama-3.1-8b-instant:")
        
        if nll_ok:
            print(f"   ğŸ† EXCELENTE! Use este modelo para:")
            print(f"      â€¢ CÃ¡lculo direto de NLL")
            print(f"      â€¢ AnÃ¡lise de perplexidade")
            print(f"      â€¢ AvaliaÃ§Ã£o de modelos")
            print(f"   ğŸ“ Script: JÃ¡ implementado neste teste")
        else:
            print(f"   âš ï¸ Use apenas para geraÃ§Ã£o de texto bÃ¡sica")
            print(f"   ğŸ”„ Para NLL, continue usando Google Gemini")
        
        return {
            "model": self.model_name,
            "basic_functionality": basic_ok,
            "logprobs_available": logprobs_ok,
            "nll_calculable": nll_ok,
            "nll_value": logprobs_result.get("nll_value") if nll_ok else None,
            "recommendation": "use_for_nll" if nll_ok else "use_basic_only"
        }
    
    async def run_comprehensive_test(self):
        """Executar teste completo do llama-3.1-8b-instant"""
        
        self.print_section(f"TESTE ABRANGENTE: {self.model_name}")
        
        print("ğŸ¯ OBJETIVO: Verificar se llama-3.1-8b-instant suporta logprobs para NLL")
        
        # Executar testes
        basic_result = await self.test_basic_functionality()
        logprobs_result = await self.test_logprobs_comprehensive()
        multiple_results = await self.test_multiple_prompts()
        
        # AnÃ¡lise final
        summary = self.analyze_final_results(basic_result, logprobs_result, multiple_results)
        
        # Salvar resultados
        output_data = {
            "model": self.model_name,
            "test_timestamp": "2024-12-19",
            "basic_test": basic_result,
            "logprobs_test": logprobs_result,
            "multiple_prompts_test": multiple_results,
            "summary": summary
        }
        
        try:
            filename = f"llama_3_1_8b_instant_nll_test.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nğŸ’¾ Resultados salvos em: {filename}")
        except Exception as e:
            print(f"âŒ Erro ao salvar: {e}")
        
        return summary

async def main():
    """FunÃ§Ã£o principal"""
    
    print("ğŸ§ª TESTE ESPECÃFICO: llama-3.1-8b-instant para NLL")
    print("=" * 60)
    
    tester = LlamaMInstantNLLTester()
    summary = await tester.run_comprehensive_test()
    
    print(f"\nğŸ Teste do llama-3.1-8b-instant concluÃ­do!")
    
    if summary.get("nll_calculable"):
        print(f"ğŸ‰ RESULTADO: âœ… MODELO SUPORTA NLL!")
        print(f"ğŸ“Š NLL obtido: {summary.get('nll_value', 'N/A')}")
    else:
        print(f"ğŸ˜” RESULTADO: âŒ Modelo nÃ£o suporta NLL")
        print(f"ğŸ’¡ Continue usando Google Gemini para NLL")

if __name__ == "__main__":
    asyncio.run(main()) 