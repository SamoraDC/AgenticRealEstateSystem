#!/usr/bin/env python3
"""
Teste Aprimorado de Acesso aos Logs para C√°lculo de NLL
Explora em detalhes os logprobs dispon√≠veis e testa m√∫ltiplos modelos
"""

import os
import json
import asyncio
import traceback
import math
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente
load_dotenv()

# Verificar bibliotecas
try:
    import google.generativeai as genai
    from google.generativeai.types import GenerationConfig
    GOOGLE_AVAILABLE = True
except ImportError:
    print("‚ùå Google Generative AI n√£o dispon√≠vel")
    GOOGLE_AVAILABLE = False

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    print("‚ùå Groq n√£o dispon√≠vel")
    GROQ_AVAILABLE = False

class EnhancedNLLTester:
    """Testador aprimorado de acesso aos logs para c√°lculo de NLL"""
    
    def __init__(self):
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        
        # Prompt mais estruturado para melhor an√°lise
        self.test_prompt = "Explain the concept of machine learning in exactly 15 words."
        
        # Modelos Groq que podem suportar logprobs
        self.groq_models_to_test = [
            "meta-llama/llama-4-scout-17b-16e-instruct",
            "llama-3.1-70b-versatile",
            "llama-3.1-8b-instant", 
            "mixtral-8x7b-32768",
            "gemma2-9b-it"
        ]
        
        self.setup_clients()
    
    def setup_clients(self):
        """Configurar clientes para APIs"""
        if GOOGLE_AVAILABLE and self.google_api_key:
            genai.configure(api_key=self.google_api_key)
            print("‚úÖ Google API configurada")
        else:
            print("‚ùå Google API n√£o configurada")
        
        if GROQ_AVAILABLE and self.groq_api_key:
            self.groq_client = Groq(api_key=self.groq_api_key)
            print("‚úÖ Groq API configurada")
        else:
            print("‚ùå Groq API n√£o configurada")
    
    def print_section(self, title: str):
        """Imprimir se√ß√£o formatada"""
        print(f"\n{'='*70}")
        print(f" {title}")
        print(f"{'='*70}")
    
    def deep_explore_object(self, obj, name: str, depth: int = 0, max_depth: int = 3):
        """Explorar profundamente objeto para encontrar logprobs"""
        if depth > max_depth:
            return
        
        indent = "  " * depth
        print(f"{indent}üîç Explorando {name} ({type(obj)})")
        
        if hasattr(obj, '__dict__'):
            for attr_name, attr_value in obj.__dict__.items():
                if not attr_name.startswith('_'):
                    print(f"{indent}  - {attr_name}: {type(attr_value)}")
                    
                    # Se cont√©m 'prob' ou 'log', explorar mais
                    if 'prob' in attr_name.lower() or 'log' in attr_name.lower():
                        print(f"{indent}    üéØ INTERESSE: {attr_name} = {attr_value}")
                        if depth < max_depth - 1:
                            self.deep_explore_object(attr_value, f"{name}.{attr_name}", depth + 1, max_depth)
        
        # Se for lista, explorar elementos
        if isinstance(obj, (list, tuple)) and len(obj) > 0:
            print(f"{indent}  üìã Lista com {len(obj)} elementos")
            if len(obj) <= 5:  # Explorar apenas listas pequenas
                for i, item in enumerate(obj):
                    if depth < max_depth - 1:
                        self.deep_explore_object(item, f"{name}[{i}]", depth + 1, max_depth)
    
    async def test_google_gemini_detailed(self) -> Dict[str, Any]:
        """Teste detalhado do Google Gemini com foco em logprobs"""
        
        self.print_section("AN√ÅLISE DETALHADA - GOOGLE GEMINI 2.0 FLASH LITE")
        
        if not GOOGLE_AVAILABLE or not self.google_api_key:
            return {"status": "error", "error": "API n√£o configurada"}
        
        try:
            model = genai.GenerativeModel('gemini-2.0-flash-lite')
            
            print(f"üìù Prompt: {self.test_prompt}")
            print("üîÑ Enviando requisi√ß√£o com configura√ß√£o para logprobs...")
            
            # Configura√ß√£o espec√≠fica para tentar obter logprobs
            generation_config = GenerationConfig(
                temperature=0.1,  # Temperatura baixa para an√°lise mais precisa
                max_output_tokens=30,
                candidate_count=1,
                stop_sequences=None,
                response_mime_type="text/plain"
            )
            
            # Fazer requisi√ß√£o
            response = model.generate_content(
                self.test_prompt,
                generation_config=generation_config
            )
            
            print("‚úÖ Resposta recebida")
            print(f"üì§ Resposta: {response.text}")
            
            result = {
                "model": "gemini-2.0-flash-lite",
                "provider": "Google",
                "status": "success",
                "response_text": response.text,
                "nll_calculable": False,
                "detailed_analysis": {}
            }
            
            # An√°lise detalhada dos candidates
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                
                print(f"\nüéØ ANALISANDO CANDIDATE DETALHADAMENTE:")
                
                # Explorar avg_logprobs
                if hasattr(candidate, 'avg_logprobs'):
                    avg_logprobs = candidate.avg_logprobs
                    print(f"üìä avg_logprobs: {avg_logprobs}")
                    result["detailed_analysis"]["avg_logprobs"] = avg_logprobs
                
                # Explorar logprobs_result em detalhes
                if hasattr(candidate, 'logprobs_result'):
                    print(f"\nüîç EXPLORANDO LOGPROBS_RESULT:")
                    logprobs_result = candidate.logprobs_result
                    
                    self.deep_explore_object(logprobs_result, "logprobs_result", max_depth=4)
                    
                    # Tentar acessar atributos espec√≠ficos do logprobs_result
                    if hasattr(logprobs_result, 'chosen_candidates'):
                        print(f"üìã chosen_candidates: {logprobs_result.chosen_candidates}")
                        result["detailed_analysis"]["chosen_candidates"] = str(logprobs_result.chosen_candidates)
                    
                    if hasattr(logprobs_result, 'top_candidates'):
                        print(f"üìã top_candidates: {logprobs_result.top_candidates}")
                        
                        # Explorar top_candidates em detalhes
                        if logprobs_result.top_candidates:
                            print(f"üîç Explorando {len(logprobs_result.top_candidates)} top_candidates:")
                            
                            total_logprob = 0.0
                            token_count = 0
                            token_details = []
                            
                            for i, candidate_item in enumerate(logprobs_result.top_candidates):
                                print(f"  Candidate {i}:")
                                self.deep_explore_object(candidate_item, f"top_candidate[{i}]", max_depth=3)
                                
                                # Tentar extrair token e logprob
                                if hasattr(candidate_item, 'token'):
                                    token = candidate_item.token
                                    print(f"    Token: '{token}'")
                                    
                                if hasattr(candidate_item, 'log_probability'):
                                    logprob = candidate_item.log_probability
                                    print(f"    Log Probability: {logprob}")
                                    
                                    total_logprob += logprob
                                    token_count += 1
                                    token_details.append({
                                        "token": getattr(candidate_item, 'token', f'token_{i}'),
                                        "logprob": logprob
                                    })
                            
                            # Se conseguimos extrair dados, calcular NLL
                            if token_count > 0:
                                nll = -total_logprob
                                perplexity = math.exp(nll / token_count)
                                
                                result.update({
                                    "nll_calculable": True,
                                    "token_count": token_count,
                                    "total_logprob": total_logprob,
                                    "nll_value": nll,
                                    "perplexity": perplexity,
                                    "token_details": token_details
                                })
                                
                                print(f"\nüìä M√âTRICAS EXTRA√çDAS:")
                                print(f"    Tokens analisados: {token_count}")
                                print(f"    Total logprob: {total_logprob:.4f}")
                                print(f"    NLL: {nll:.4f}")
                                print(f"    Perplexity: {perplexity:.4f}")
                
                # An√°lise do content para tokens individuais
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    print(f"\nüîç ANALISANDO CONTENT PARTS:")
                    for i, part in enumerate(candidate.content.parts):
                        print(f"  Part {i}: {type(part)}")
                        self.deep_explore_object(part, f"part[{i}]", max_depth=2)
            
            return result
            
        except Exception as e:
            print(f"‚ùå Erro detalhado: {e}")
            traceback.print_exc()
            return {"status": "error", "error": str(e)}
    
    async def test_groq_models_comprehensive(self) -> List[Dict[str, Any]]:
        """Testar m√∫ltiplos modelos Groq para encontrar suporte a logprobs"""
        
        self.print_section("TESTE ABRANGENTE - MODELOS GROQ")
        
        if not GROQ_AVAILABLE or not self.groq_api_key:
            return [{"status": "error", "error": "Groq n√£o configurado"}]
        
        results = []
        
        for model_name in self.groq_models_to_test:
            print(f"\nü§ñ Testando modelo: {model_name}")
            
            try:
                # Primeiro, testar sem logprobs para ver se o modelo funciona
                print("  üîç Testando funcionalidade b√°sica...")
                
                basic_response = self.groq_client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": self.test_prompt}],
                    temperature=0.1,
                    max_tokens=30
                )
                
                response_text = basic_response.choices[0].message.content
                print(f"  ‚úÖ Funcionalidade b√°sica OK: {response_text[:50]}...")
                
                # Agora tentar com logprobs
                print("  üîç Testando logprobs...")
                
                try:
                    logprobs_response = self.groq_client.chat.completions.create(
                        model=model_name,
                        messages=[{"role": "user", "content": self.test_prompt}],
                        temperature=0.1,
                        max_tokens=30,
                        logprobs=True,
                        top_logprobs=5
                    )
                    
                    print("  ‚úÖ Logprobs suportados!")
                    
                    # Analisar logprobs
                    choice = logprobs_response.choices[0]
                    result = {
                        "model": model_name,
                        "provider": "Groq",
                        "status": "success",
                        "response_text": choice.message.content,
                        "logprobs_available": True,
                        "nll_calculable": False
                    }
                    
                    if hasattr(choice, 'logprobs') and choice.logprobs:
                        logprobs = choice.logprobs
                        
                        if hasattr(logprobs, 'content') and logprobs.content:
                            print(f"    üìä {len(logprobs.content)} tokens com logprobs")
                            
                            total_logprob = 0.0
                            token_details = []
                            
                            for token_logprob in logprobs.content:
                                token = token_logprob.token
                                logprob = token_logprob.logprob
                                
                                total_logprob += logprob
                                token_details.append({
                                    "token": token,
                                    "logprob": logprob
                                })
                            
                            # Calcular NLL
                            nll = -total_logprob
                            perplexity = math.exp(nll / len(logprobs.content))
                            
                            result.update({
                                "nll_calculable": True,
                                "token_count": len(logprobs.content),
                                "total_logprob": total_logprob,
                                "nll_value": nll,
                                "perplexity": perplexity,
                                "token_details": token_details[:5]  # Primeiros 5 tokens
                            })
                            
                            print(f"    üìä NLL: {nll:.4f}, Perplexity: {perplexity:.4f}")
                    
                    results.append(result)
                    
                except Exception as logprob_error:
                    print(f"  ‚ùå Logprobs n√£o suportados: {logprob_error}")
                    results.append({
                        "model": model_name,
                        "provider": "Groq",
                        "status": "partial",
                        "basic_functionality": True,
                        "logprobs_available": False,
                        "logprobs_error": str(logprob_error)
                    })
                    
            except Exception as e:
                print(f"  ‚ùå Modelo n√£o funciona: {e}")
                results.append({
                    "model": model_name,
                    "provider": "Groq",
                    "status": "error",
                    "error": str(e)
                })
        
        return results
    
    def analyze_comprehensive_results(self, google_result: Dict, groq_results: List[Dict]):
        """An√°lise abrangente de todos os resultados"""
        
        self.print_section("AN√ÅLISE ABRANGENTE DOS RESULTADOS")
        
        print("üìä RESUMO COMPLETO:\n")
        
        # An√°lise Google
        print("üîµ GOOGLE GEMINI 2.0 FLASH LITE:")
        if google_result["status"] == "success":
            print(f"   ‚úÖ Funcionando")
            print(f"   üìä NLL Calcul√°vel: {'‚úÖ' if google_result.get('nll_calculable') else '‚ùå'}")
            
            if google_result.get('nll_calculable'):
                print(f"   üìà NLL: {google_result['nll_value']:.4f}")
                print(f"   üìà Perplexity: {google_result['perplexity']:.4f}")
                print(f"   üî¢ Tokens: {google_result['token_count']}")
        else:
            print(f"   ‚ùå Erro: {google_result.get('error')}")
        
        # An√°lise Groq
        print(f"\nüü¢ GROQ MODELS ({len(groq_results)} testados):")
        
        working_models = []
        logprobs_models = []
        nll_models = []
        
        for result in groq_results:
            model = result["model"]
            status = result["status"]
            
            print(f"\n   ü§ñ {model}:")
            
            if status == "success":
                print(f"      ‚úÖ Totalmente funcional")
                working_models.append(model)
                
                if result.get("logprobs_available"):
                    print(f"      üìä Logprobs: ‚úÖ")
                    logprobs_models.append(model)
                    
                    if result.get("nll_calculable"):
                        print(f"      üìà NLL: ‚úÖ ({result['nll_value']:.4f})")
                        nll_models.append((model, result['nll_value']))
                    else:
                        print(f"      üìà NLL: ‚ùå")
                else:
                    print(f"      üìä Logprobs: ‚ùå")
                    
            elif status == "partial":
                print(f"      ‚ö†Ô∏è Funcional b√°sico, sem logprobs")
                working_models.append(model)
                
            else:
                print(f"      ‚ùå N√£o funcional: {result.get('error', 'Erro desconhecido')}")
        
        # Estat√≠sticas finais
        print(f"\nüìà ESTAT√çSTICAS FINAIS:")
        print(f"   ü§ñ Modelos funcionais: {len(working_models)}/{len(groq_results) + 1}")
        print(f"   üìä Com logprobs: {len(logprobs_models) + (1 if google_result.get('nll_calculable') else 0)}")
        print(f"   üìà NLL calcul√°vel: {len(nll_models) + (1 if google_result.get('nll_calculable') else 0)}")
        
        # Recomenda√ß√µes espec√≠ficas
        print(f"\nüí° RECOMENDA√á√ïES ESPEC√çFICAS:")
        
        if nll_models:
            print(f"‚úÖ MELHORES OP√á√ïES PARA NLL:")
            for model, nll_value in sorted(nll_models, key=lambda x: x[1]):
                print(f"   üèÜ {model}: NLL = {nll_value:.4f}")
        
        if google_result.get('nll_calculable'):
            print(f"‚úÖ Google Gemini: NLL = {google_result['nll_value']:.4f}")
        
        if logprobs_models:
            print(f"\n‚úÖ MODELOS GROQ COM LOGPROBS:")
            for model in logprobs_models:
                print(f"   üìä {model}")
        
        if not nll_models and not google_result.get('nll_calculable'):
            print(f"\n‚ö†Ô∏è ALTERNATIVAS RECOMENDADAS:")
            print(f"   1. Use modelos locais (Ollama/Hugging Face)")
            print(f"   2. APIs especializadas (OpenAI com logprobs)")
            print(f"   3. Estimativas via m√∫ltiplo sampling")
    
    async def run_comprehensive_tests(self):
        """Executar todos os testes abrangentes"""
        
        print("üß™ TESTE ABRANGENTE DE ACESSO AOS LOGS NLL")
        print("=" * 70)
        
        # Testar Google Gemini em detalhes
        google_result = await self.test_google_gemini_detailed()
        
        # Testar m√∫ltiplos modelos Groq
        groq_results = await self.test_groq_models_comprehensive()
        
        # An√°lise final
        self.analyze_comprehensive_results(google_result, groq_results)
        
        # Salvar resultados
        all_results = {
            "google_gemini": google_result,
            "groq_models": groq_results,
            "test_prompt": self.test_prompt,
            "timestamp": "2024-12-19"
        }
        
        try:
            with open("comprehensive_nll_results.json", 'w', encoding='utf-8') as f:
                json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nüíæ Resultados detalhados salvos em: comprehensive_nll_results.json")
        except Exception as e:
            print(f"‚ùå Erro ao salvar: {e}")
        
        return all_results

async def main():
    """Fun√ß√£o principal"""
    
    tester = EnhancedNLLTester()
    await tester.run_comprehensive_tests()
    
    print(f"\nüèÅ Teste abrangente conclu√≠do!")

if __name__ == "__main__":
    asyncio.run(main()) 