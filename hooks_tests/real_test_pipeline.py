#!/usr/bin/env python3
"""
Pipeline de Testes REAL - Real Estate Assistant
Pipeline completo integrado com sistema agÃªntico real usando dados Mock
"""

import asyncio
import json
import time
import requests
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from pathlib import Path
import sys

# Adicionar diretÃ³rio raiz ao path
sys.path.append(str(Path(__file__).parent.parent))

# Importar componentes reais
from real_stress_testing import RealSystemStressTester, RealVirtualUser
from real_conversation_hooks import (
    RealConversationAnalyzer, ProductionConversationMonitor,
    RealConversationEvent, RealAgentType, RealConversationPhase
)
from app.orchestration.swarm import SwarmOrchestrator
from app.utils.logging import get_logger, log_agent_action, log_api_call, log_error
from config.settings import get_settings

@dataclass
class RealTestScenario:
    """CenÃ¡rio de teste para sistema real"""
    name: str
    description: str
    virtual_users: List[RealVirtualUser]
    conversation_scripts: List[List[Dict[str, Any]]]
    expected_outcomes: Dict[str, Any]
    performance_thresholds: Dict[str, float]
    mock_data_requirements: List[str]

@dataclass
class RealTestResults:
    """Resultados de testes do sistema real"""
    scenario_name: str
    stress_test_results: Dict[str, Any]
    conversation_analysis: Dict[str, Any]
    integration_metrics: Dict[str, Any]
    mock_data_validation: Dict[str, Any]
    system_health: Dict[str, Any]
    overall_grade: str
    recommendations: List[str]
    timestamp: datetime

class RealEstateTestPipeline:
    """Pipeline integrado de testes para sistema agÃªntico real"""
    
    def __init__(self):
        self.logger = get_logger("real_test_pipeline")
        self.stress_tester = RealSystemStressTester()
        self.conversation_monitor = ProductionConversationMonitor()
        self.swarm = SwarmOrchestrator()
        self.test_scenarios = self._create_real_test_scenarios()
        self.results_history: List[RealTestResults] = []
        self.mock_api_base = "http://localhost:8000"
        
        # Verificar sistema
        self._verify_system_components()
    
    def _verify_system_components(self):
        """Verifica componentes do sistema real"""
        try:
            # Verificar configuraÃ§Ãµes
            settings = get_settings()
            self.logger.info(f"âœ… ConfiguraÃ§Ãµes carregadas")
            
            # Verificar OpenRouter
            if settings.apis.openrouter_key and settings.apis.openrouter_key != "your_openrouter_api_key_here":
                self.logger.info(f"âœ… OpenRouter configurado")
            else:
                self.logger.warning(f"âš ï¸ OpenRouter nÃ£o configurado")
            
            # Verificar sistema Mock
            response = requests.get(f"{self.mock_api_base}/api/properties/search", timeout=5)
            if response.status_code == 200:
                data = response.json()
                properties_count = len(data.get("properties", []))
                self.logger.info(f"âœ… Sistema Mock disponÃ­vel - {properties_count} propriedades")
            else:
                self.logger.warning(f"âš ï¸ Sistema Mock indisponÃ­vel")
                
        except Exception as e:
            self.logger.error(f"âŒ Erro na verificaÃ§Ã£o do sistema: {e}")
    
    def _create_real_test_scenarios(self) -> List[RealTestScenario]:
        """Cria cenÃ¡rios de teste para sistema real"""
        
        scenarios = [
            RealTestScenario(
                name="real_basic_user_journey",
                description="Jornada bÃ¡sica com sistema agÃªntico real e dados Mock",
                virtual_users=self.stress_tester.virtual_users[:2],
                conversation_scripts=[
                    [
                        {"user_input": "Hi, I'm looking for an apartment", "agent_type": "search_agent", "phase": "greeting"},
                        {"user_input": "Tell me about properties in Miami", "agent_type": "search_agent", "phase": "search_criteria"},
                        {"user_input": "What about the property at 501 Sw 6th Ct?", "agent_type": "property_agent", "phase": "property_details"},
                        {"user_input": "Can I schedule a viewing?", "agent_type": "scheduling_agent", "phase": "scheduling"}
                    ]
                ],
                expected_outcomes={
                    "min_interactions": 4,
                    "agents_used": 3,
                    "phases_covered": 4,
                    "mock_data_accessed": True
                },
                performance_thresholds={
                    "max_response_time": 8.0,  # Mais tolerante para sistema real
                    "min_success_rate": 85.0
                },
                mock_data_requirements=["properties", "addresses", "pricing"]
            ),
            
            RealTestScenario(
                name="real_mock_data_integration",
                description="Teste especÃ­fico de integraÃ§Ã£o com dados Mock",
                virtual_users=self.stress_tester.virtual_users[2:4],
                conversation_scripts=[
                    [
                        {"user_input": "Show me apartments under $2000", "agent_type": "search_agent", "phase": "search_criteria"},
                        {"user_input": "What properties do you have in Miami-Dade?", "agent_type": "search_agent", "phase": "search_criteria"},
                        {"user_input": "Tell me about 1-bedroom apartments", "agent_type": "property_agent", "phase": "property_details"},
                        {"user_input": "What's the square footage of available units?", "agent_type": "property_agent", "phase": "property_details"},
                        {"user_input": "Are there any properties built after 1920?", "agent_type": "search_agent", "phase": "search_criteria"},
                        {"user_input": "I want to schedule viewings for the best options", "agent_type": "scheduling_agent", "phase": "scheduling"}
                    ]
                ],
                expected_outcomes={
                    "min_interactions": 6,
                    "agents_used": 3,
                    "mock_data_queries": 5,
                    "specific_property_mentions": True
                },
                performance_thresholds={
                    "max_response_time": 10.0,
                    "min_success_rate": 80.0
                },
                mock_data_requirements=["properties", "search_filters", "property_details"]
            ),
            
            RealTestScenario(
                name="real_high_volume_concurrent",
                description="Teste de alto volume com sistema real",
                virtual_users=self.stress_tester.virtual_users,
                conversation_scripts=[],  # Gerado dinamicamente
                expected_outcomes={
                    "concurrent_users": 5,
                    "total_interactions": 25,
                    "system_stability": True
                },
                performance_thresholds={
                    "max_response_time": 12.0,  # Mais tolerante para alta carga
                    "min_success_rate": 75.0
                },
                mock_data_requirements=["properties", "concurrent_access"]
            ),
            
            RealTestScenario(
                name="real_error_handling_resilience",
                description="Teste de resiliÃªncia e tratamento de erros do sistema real",
                virtual_users=self.stress_tester.virtual_users[:1],
                conversation_scripts=[
                    [
                        {"user_input": "", "agent_type": "search_agent", "phase": "error_handling"},  # Input vazio
                        {"user_input": "I want a 50-bedroom mansion for $100", "agent_type": "search_agent", "phase": "search_criteria"},  # CritÃ©rio impossÃ­vel
                        {"user_input": "What's the weather like today?", "agent_type": "search_agent", "phase": "error_handling"},  # Fora do escopo
                        {"user_input": "Show me property ID 99999999", "agent_type": "property_agent", "phase": "property_details"},  # ID inexistente
                        {"user_input": "Schedule viewing for yesterday", "agent_type": "scheduling_agent", "phase": "error_handling"}  # Data invÃ¡lida
                    ]
                ],
                expected_outcomes={
                    "error_handling": True,
                    "graceful_degradation": True,
                    "fallback_responses": True
                },
                performance_thresholds={
                    "max_response_time": 15.0,
                    "min_success_rate": 60.0  # Menor devido aos casos de erro
                },
                mock_data_requirements=["error_responses", "fallback_data"]
            )
        ]
        
        return scenarios
    
    async def validate_mock_data_integration(self, scenario: RealTestScenario) -> Dict[str, Any]:
        """Valida integraÃ§Ã£o com dados Mock"""
        validation_results = {
            "mock_api_accessible": False,
            "properties_available": 0,
            "data_quality": "unknown",
            "required_data_present": False,
            "api_response_time": 0.0
        }
        
        try:
            # Testar acesso Ã  API Mock
            start_time = time.time()
            response = requests.get(f"{self.mock_api_base}/api/properties/search", timeout=10)
            api_time = time.time() - start_time
            
            validation_results["api_response_time"] = api_time
            
            if response.status_code == 200:
                validation_results["mock_api_accessible"] = True
                data = response.json()
                properties = data.get("properties", [])
                validation_results["properties_available"] = len(properties)
                
                # Verificar qualidade dos dados
                if properties:
                    sample_property = properties[0]
                    required_fields = ["formattedAddress", "price", "bedrooms", "bathrooms", "squareFootage"]
                    present_fields = sum(1 for field in required_fields if field in sample_property)
                    
                    if present_fields >= 4:
                        validation_results["data_quality"] = "good"
                    elif present_fields >= 2:
                        validation_results["data_quality"] = "fair"
                    else:
                        validation_results["data_quality"] = "poor"
                
                # Verificar se dados necessÃ¡rios estÃ£o presentes
                validation_results["required_data_present"] = len(properties) > 0
                
                self.logger.info(f"âœ… Mock data validation successful - {len(properties)} properties")
            else:
                self.logger.warning(f"âš ï¸ Mock API returned status {response.status_code}")
                
        except Exception as e:
            self.logger.error(f"âŒ Mock data validation failed: {e}")
            validation_results["error"] = str(e)
        
        return validation_results
    
    async def run_real_scenario(self, scenario: RealTestScenario) -> RealTestResults:
        """Executa cenÃ¡rio de teste com sistema real"""
        
        self.logger.info(f"ğŸ¯ Executando cenÃ¡rio real: {scenario.name}")
        self.logger.info(f"ğŸ“ {scenario.description}")
        
        start_time = time.time()
        
        # 1. Validar integraÃ§Ã£o com dados Mock
        self.logger.info("   ğŸ“Š Validando dados Mock...")
        mock_validation = await self.validate_mock_data_integration(scenario)
        
        # 2. Iniciar monitoramento de conversas
        self.conversation_monitor.start_monitoring()
        
        # 3. Executar stress test com sistema real
        if scenario.name == "real_high_volume_concurrent":
            self.logger.info("   ğŸš€ Executando stress test de alto volume...")
            stress_results = await self.stress_tester.run_real_stress_test(
                concurrent_users=len(scenario.virtual_users),
                questions_per_user=5
            )
        else:
            self.logger.info("   ğŸ”„ Executando stress test controlado...")
            stress_results = await self.stress_tester.run_real_stress_test(
                concurrent_users=len(scenario.virtual_users),
                questions_per_user=3
            )
        
        # 4. Executar scripts de conversa especÃ­ficos
        if scenario.conversation_scripts:
            self.logger.info("   ğŸ’¬ Executando scripts de conversa...")
            for i, script in enumerate(scenario.conversation_scripts):
                session_id = f"{scenario.name}_script_{i}"
                await self._execute_conversation_script(session_id, script)
        
        # 5. Parar monitoramento e analisar
        self.conversation_monitor.stop_monitoring()
        conversation_analysis = self.conversation_monitor.get_live_statistics()
        
        # 6. Calcular mÃ©tricas de integraÃ§Ã£o
        integration_metrics = self._calculate_real_integration_metrics(
            scenario, stress_results, conversation_analysis, mock_validation
        )
        
        # 7. Avaliar saÃºde do sistema
        system_health = self._assess_real_system_health(
            stress_results, conversation_analysis, mock_validation
        )
        
        # 8. Determinar nota geral
        overall_grade = self._calculate_real_overall_grade(
            scenario, stress_results, conversation_analysis, integration_metrics, system_health
        )
        
        # 9. Gerar recomendaÃ§Ãµes
        recommendations = self._generate_real_recommendations(
            scenario, stress_results, conversation_analysis, integration_metrics, system_health
        )
        
        execution_time = time.time() - start_time
        self.logger.info(f"   âœ… CenÃ¡rio concluÃ­do em {execution_time:.2f}s")
        
        # Criar resultado
        result = RealTestResults(
            scenario_name=scenario.name,
            stress_test_results=stress_results,
            conversation_analysis=conversation_analysis,
            integration_metrics=integration_metrics,
            mock_data_validation=mock_validation,
            system_health=system_health,
            overall_grade=overall_grade,
            recommendations=recommendations,
            timestamp=datetime.now()
        )
        
        self.results_history.append(result)
        return result
    
    async def _execute_conversation_script(self, session_id: str, script: List[Dict[str, Any]]):
        """Executa script de conversa usando sistema real"""
        
        for i, step in enumerate(script):
            try:
                user_input = step["user_input"]
                expected_agent = step.get("agent_type", "search_agent")
                
                if not user_input.strip():  # Input vazio para teste de erro
                    user_input = "Hello"
                
                # Executar atravÃ©s do SwarmOrchestrator real
                message_data = {
                    "content": user_input,
                    "session_id": session_id,
                    "context": {
                        "data_mode": "mock",
                        "property_context": {
                            "formattedAddress": "501 Sw 6th Ct, Apt 215, Miami, FL 33130",
                            "price": 1600,
                            "bedrooms": 1,
                            "bathrooms": 1,
                            "squareFootage": 600
                        }
                    }
                }
                
                start_time = time.time()
                response = await self.swarm.process_message(message_data)
                response_time = time.time() - start_time
                
                # Extrair resposta
                if "messages" in response and response["messages"]:
                    agent_response = response["messages"][-1].content
                    success = True
                else:
                    agent_response = "No response received"
                    success = False
                
                # Monitorar atravÃ©s do sistema de hooks
                await self.conversation_monitor.monitor_conversation(
                    session_id=session_id,
                    user_input=user_input,
                    agent_response=agent_response,
                    agent_type=expected_agent,
                    response_time=response_time,
                    success=success,
                    context={"script_step": i, "scenario": "test_script"}
                )
                
                # Delay entre interaÃ§Ãµes
                await asyncio.sleep(0.5)
                
            except Exception as e:
                self.logger.error(f"Erro no script step {i}: {e}")
                
                # Monitorar erro
                await self.conversation_monitor.monitor_conversation(
                    session_id=session_id,
                    user_input=step.get("user_input", ""),
                    agent_response=f"Error: {str(e)}",
                    agent_type=step.get("agent_type", "search_agent"),
                    response_time=0.0,
                    success=False,
                    context={"error": str(e), "script_step": i}
                )
    
    def _calculate_real_integration_metrics(self, scenario: RealTestScenario, 
                                          stress_results: Dict, conversation_analysis: Dict,
                                          mock_validation: Dict) -> Dict[str, Any]:
        """Calcula mÃ©tricas de integraÃ§Ã£o para sistema real"""
        
        metrics = {
            "system_coherence": 0.0,
            "agent_coordination": 0.0,
            "response_consistency": 0.0,
            "error_resilience": 0.0,
            "mock_data_integration": 0.0,
            "user_experience_score": 0.0
        }
        
        # CoerÃªncia do sistema
        if stress_results and "execution_stats" in stress_results:
            success_rate = stress_results["execution_stats"]["success_rate"]
            metrics["system_coherence"] = min(success_rate / 100.0, 1.0)
        
        # CoordenaÃ§Ã£o entre agentes
        if conversation_analysis and "agent_transition_patterns" in conversation_analysis:
            transitions = conversation_analysis["agent_transition_patterns"]
            total_transitions = transitions.get("total_transitions", 0)
            unique_patterns = transitions.get("unique_patterns", 0)
            if total_transitions > 0:
                metrics["agent_coordination"] = min(unique_patterns / 3.0, 1.0)  # Esperamos pelo menos 3 padrÃµes
        
        # ConsistÃªncia de resposta
        if stress_results and "execution_stats" in stress_results:
            avg_time = stress_results["execution_stats"]["average_response_time"]
            threshold = scenario.performance_thresholds.get("max_response_time", 8.0)
            metrics["response_consistency"] = max(0.0, 1.0 - (avg_time / threshold))
        
        # ResiliÃªncia a erros
        if scenario.name == "real_error_handling_resilience":
            # Para cenÃ¡rios de erro, avaliar se o sistema se manteve estÃ¡vel
            metrics["error_resilience"] = 0.8 if metrics["system_coherence"] > 0.5 else 0.3
        else:
            metrics["error_resilience"] = metrics["system_coherence"]
        
        # IntegraÃ§Ã£o com dados Mock
        if mock_validation.get("mock_api_accessible", False):
            mock_score = 0.0
            if mock_validation.get("properties_available", 0) > 0:
                mock_score += 0.4
            if mock_validation.get("data_quality") == "good":
                mock_score += 0.4
            elif mock_validation.get("data_quality") == "fair":
                mock_score += 0.2
            if mock_validation.get("api_response_time", 10) < 5.0:
                mock_score += 0.2
            metrics["mock_data_integration"] = mock_score
        
        # Score de experiÃªncia do usuÃ¡rio (mÃ©dia ponderada)
        weights = [0.25, 0.2, 0.2, 0.15, 0.2]  # Pesos para cada mÃ©trica
        values = [
            metrics["system_coherence"],
            metrics["agent_coordination"], 
            metrics["response_consistency"],
            metrics["error_resilience"],
            metrics["mock_data_integration"]
        ]
        metrics["user_experience_score"] = sum(w * v for w, v in zip(weights, values))
        
        return metrics
    
    def _assess_real_system_health(self, stress_results: Dict, conversation_analysis: Dict,
                                 mock_validation: Dict) -> Dict[str, Any]:
        """Avalia saÃºde do sistema real"""
        
        health = {
            "overall_status": "unknown",
            "components": {
                "stress_testing": "unknown",
                "conversation_flow": "unknown", 
                "mock_data": "unknown",
                "agent_coordination": "unknown"
            },
            "metrics": {},
            "issues": []
        }
        
        # Avaliar stress testing
        if stress_results and "execution_stats" in stress_results:
            success_rate = stress_results["execution_stats"]["success_rate"]
            if success_rate >= 90:
                health["components"]["stress_testing"] = "healthy"
            elif success_rate >= 70:
                health["components"]["stress_testing"] = "warning"
            else:
                health["components"]["stress_testing"] = "critical"
                health["issues"].append(f"Low stress test success rate: {success_rate:.1f}%")
        
        # Avaliar fluxo de conversa
        if conversation_analysis and "overall_success_rate" in conversation_analysis:
            conv_success = conversation_analysis["overall_success_rate"]
            if conv_success >= 85:
                health["components"]["conversation_flow"] = "healthy"
            elif conv_success >= 65:
                health["components"]["conversation_flow"] = "warning"
            else:
                health["components"]["conversation_flow"] = "critical"
                health["issues"].append(f"Poor conversation success rate: {conv_success:.1f}%")
        
        # Avaliar dados Mock
        if mock_validation.get("mock_api_accessible", False):
            if mock_validation.get("data_quality") == "good":
                health["components"]["mock_data"] = "healthy"
            elif mock_validation.get("data_quality") == "fair":
                health["components"]["mock_data"] = "warning"
            else:
                health["components"]["mock_data"] = "critical"
        else:
            health["components"]["mock_data"] = "critical"
            health["issues"].append("Mock API not accessible")
        
        # Avaliar coordenaÃ§Ã£o de agentes
        if conversation_analysis and "agent_transition_patterns" in conversation_analysis:
            transitions = conversation_analysis["agent_transition_patterns"]
            if transitions.get("total_transitions", 0) > 0:
                health["components"]["agent_coordination"] = "healthy"
            else:
                health["components"]["agent_coordination"] = "warning"
                health["issues"].append("No agent transitions detected")
        
        # Determinar status geral
        component_scores = {
            "healthy": 1.0,
            "warning": 0.6,
            "critical": 0.2,
            "unknown": 0.0
        }
        
        total_score = sum(component_scores.get(status, 0) for status in health["components"].values())
        avg_score = total_score / len(health["components"])
        
        if avg_score >= 0.8:
            health["overall_status"] = "healthy"
        elif avg_score >= 0.6:
            health["overall_status"] = "warning"
        else:
            health["overall_status"] = "critical"
        
        health["metrics"]["component_average"] = avg_score
        health["metrics"]["total_issues"] = len(health["issues"])
        
        return health
    
    def _calculate_real_overall_grade(self, scenario: RealTestScenario, stress_results: Dict,
                                    conversation_analysis: Dict, integration_metrics: Dict,
                                    system_health: Dict) -> str:
        """Calcula nota geral para sistema real"""
        
        # Score de experiÃªncia do usuÃ¡rio
        ux_score = integration_metrics.get("user_experience_score", 0.0)
        
        # Score de saÃºde do sistema
        health_score = system_health["metrics"].get("component_average", 0.0)
        
        # Verificar compliance com thresholds
        threshold_compliance = 0.0
        if stress_results and "execution_stats" in stress_results:
            success_rate = stress_results["execution_stats"]["success_rate"]
            avg_response_time = stress_results["execution_stats"]["average_response_time"]
            
            min_success = scenario.performance_thresholds.get("min_success_rate", 75.0)
            max_time = scenario.performance_thresholds.get("max_response_time", 8.0)
            
            success_compliance = min(success_rate / min_success, 1.0)
            time_compliance = min(max_time / avg_response_time, 1.0)
            
            threshold_compliance = (success_compliance + time_compliance) / 2.0
        
        # Calcular nota final (mais tolerante para sistema real)
        final_score = (ux_score * 0.4) + (health_score * 0.3) + (threshold_compliance * 0.3)
        
        if final_score >= 0.85:
            return "A+ (Excelente para Sistema Real)"
        elif final_score >= 0.75:
            return "A (Muito Bom para Sistema Real)"
        elif final_score >= 0.65:
            return "B (Bom para Sistema Real)"
        elif final_score >= 0.55:
            return "C (SatisfatÃ³rio para Sistema Real)"
        else:
            return "D (Precisa Melhorar)"
    
    def _generate_real_recommendations(self, scenario: RealTestScenario, stress_results: Dict,
                                     conversation_analysis: Dict, integration_metrics: Dict,
                                     system_health: Dict) -> List[str]:
        """Gera recomendaÃ§Ãµes para sistema real"""
        
        recommendations = []
        
        # RecomendaÃ§Ãµes baseadas em saÃºde do sistema
        if system_health["overall_status"] == "critical":
            recommendations.append("ğŸš¨ CRÃTICO: Revisar configuraÃ§Ã£o do sistema imediatamente")
        elif system_health["overall_status"] == "warning":
            recommendations.append("âš ï¸ Monitorar sistema de perto - possÃ­veis problemas")
        
        # RecomendaÃ§Ãµes especÃ­ficas por componente
        for component, status in system_health["components"].items():
            if status == "critical":
                if component == "mock_data":
                    recommendations.append("ğŸ”§ Verificar conectividade com API Mock")
                elif component == "stress_testing":
                    recommendations.append("âš¡ Otimizar performance do sistema agÃªntico")
                elif component == "conversation_flow":
                    recommendations.append("ğŸ’¬ Melhorar fluxo de conversa entre agentes")
                elif component == "agent_coordination":
                    recommendations.append("ğŸ¤ Implementar melhor coordenaÃ§Ã£o entre agentes")
        
        # RecomendaÃ§Ãµes baseadas em mÃ©tricas
        mock_score = integration_metrics.get("mock_data_integration", 0.0)
        if mock_score < 0.7:
            recommendations.append("ğŸ“Š Melhorar integraÃ§Ã£o com dados Mock")
        
        ux_score = integration_metrics.get("user_experience_score", 0.0)
        if ux_score < 0.6:
            recommendations.append("ğŸ‘¤ Focar em melhorar experiÃªncia do usuÃ¡rio")
        
        # RecomendaÃ§Ãµes baseadas em performance
        if stress_results and "execution_stats" in stress_results:
            avg_time = stress_results["execution_stats"]["average_response_time"]
            if avg_time > 8.0:
                recommendations.append("â±ï¸ Otimizar tempo de resposta dos agentes")
            
            success_rate = stress_results["execution_stats"]["success_rate"]
            if success_rate < 80.0:
                recommendations.append("ğŸ¯ Melhorar taxa de sucesso geral")
        
        # RecomendaÃ§Ãµes especÃ­ficas do cenÃ¡rio
        if scenario.name == "real_error_handling_resilience":
            recommendations.append("ğŸ›¡ï¸ Implementar tratamento de erro mais robusto")
        elif scenario.name == "real_high_volume_concurrent":
            recommendations.append("ğŸ“ˆ Considerar otimizaÃ§Ãµes para alta carga")
        
        if not recommendations:
            recommendations.append("âœ… Sistema funcionando bem - manter monitoramento")
        
        return recommendations
    
    def generate_real_pipeline_report(self, results: List[RealTestResults]) -> str:
        """Gera relatÃ³rio consolidado do pipeline real"""
        
        if not results:
            return "âŒ Nenhum resultado disponÃ­vel para relatÃ³rio"
        
        report = f"""
ğŸ  RELATÃ“RIO DO PIPELINE DE TESTES REAL - REAL ESTATE ASSISTANT
{'='*75}

ğŸ¯ RESUMO EXECUTIVO:
â€¢ Total de CenÃ¡rios Testados: {len(results)}
â€¢ Sistema: AgÃªntico Real (OpenRouter + Ollama + Mock Data)
â€¢ Data da ExecuÃ§Ã£o: {results[-1].timestamp.strftime('%Y-%m-%d %H:%M:%S')}
â€¢ DuraÃ§Ã£o Total: {sum((r.timestamp - results[0].timestamp).total_seconds() for r in results[1:]):.1f}s

ğŸ“Š RESULTADOS POR CENÃRIO:
"""
        
        for result in results:
            report += f"""
ğŸ“‹ {result.scenario_name.upper().replace('_', ' ')}:
   â€¢ Nota Geral: {result.overall_grade}
   â€¢ Score UX: {result.integration_metrics.get('user_experience_score', 0):.2f}
   â€¢ SaÃºde do Sistema: {result.system_health.get('overall_status', 'unknown').upper()}
   â€¢ Taxa de Sucesso: {result.stress_test_results.get('execution_stats', {}).get('success_rate', 0):.1f}%
   â€¢ Tempo MÃ©dio: {result.stress_test_results.get('execution_stats', {}).get('average_response_time', 0):.2f}s
   â€¢ Mock Data Score: {result.integration_metrics.get('mock_data_integration', 0):.2f}
"""
        
        # Calcular mÃ©tricas agregadas
        avg_ux_score = sum(r.integration_metrics.get('user_experience_score', 0) for r in results) / len(results)
        avg_success_rate = sum(r.stress_test_results.get('execution_stats', {}).get('success_rate', 0) for r in results) / len(results)
        avg_mock_score = sum(r.integration_metrics.get('mock_data_integration', 0) for r in results) / len(results)
        
        # Contar status de saÃºde
        health_counts = {}
        for result in results:
            status = result.system_health.get('overall_status', 'unknown')
            health_counts[status] = health_counts.get(status, 0) + 1
        
        report += f"""
ğŸ“ˆ MÃ‰TRICAS AGREGADAS DO SISTEMA REAL:
â€¢ Score UX MÃ©dio: {avg_ux_score:.2f}
â€¢ Taxa de Sucesso MÃ©dia: {avg_success_rate:.1f}%
â€¢ Score Mock Data MÃ©dio: {avg_mock_score:.2f}
â€¢ CenÃ¡rios com Nota A+/A: {sum(1 for r in results if r.overall_grade.startswith('A'))}/{len(results)}

ğŸ¥ SAÃšDE DO SISTEMA:
"""
        for status, count in health_counts.items():
            report += f"â€¢ {status.upper()}: {count} cenÃ¡rios\n"
        
        report += f"""
ğŸ”§ PRINCIPAIS RECOMENDAÃ‡Ã•ES:
"""
        
        # Compilar todas as recomendaÃ§Ãµes
        all_recommendations = []
        for result in results:
            all_recommendations.extend(result.recommendations)
        
        # Contar recomendaÃ§Ãµes mais comuns
        rec_counts = {}
        for rec in all_recommendations:
            rec_counts[rec] = rec_counts.get(rec, 0) + 1
        
        # Mostrar top 5 recomendaÃ§Ãµes
        top_recommendations = sorted(rec_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        for rec, count in top_recommendations:
            report += f"â€¢ {rec} (mencionado {count}x)\n"
        
        # Determinar status geral do sistema real
        healthy_count = health_counts.get('healthy', 0)
        warning_count = health_counts.get('warning', 0)
        critical_count = health_counts.get('critical', 0)
        
        if critical_count > 0:
            status = "ğŸ”´ SISTEMA REAL PRECISA DE ATENÃ‡ÃƒO CRÃTICA"
        elif warning_count > healthy_count:
            status = "ğŸŸ¡ SISTEMA REAL ESTÃVEL - MONITORAMENTO RECOMENDADO"
        elif avg_success_rate >= 80 and avg_ux_score >= 0.7:
            status = "ğŸŸ¢ SISTEMA REAL EM EXCELENTE ESTADO"
        else:
            status = "ğŸŸ¡ SISTEMA REAL FUNCIONANDO - MELHORIAS RECOMENDADAS"
        
        report += f"\n{status}\n"
        
        # InformaÃ§Ãµes especÃ­ficas do sistema real
        report += f"""
ğŸ”— INTEGRAÃ‡ÃƒO COM SISTEMA REAL:
â€¢ OpenRouter/Ollama: Integrado e testado
â€¢ Dados Mock: Validados e acessÃ­veis
â€¢ Hooks de Conversa: Ativos e monitorando
â€¢ SwarmOrchestrator: Funcionando com agentes reais
â€¢ Logs: DisponÃ­veis em logs/

ğŸ“Š PRÃ“XIMOS PASSOS PARA PRODUÃ‡ÃƒO:
â€¢ Monitorar mÃ©tricas continuamente
â€¢ Executar testes semanais
â€¢ Configurar alertas automÃ¡ticos
â€¢ Expandir cenÃ¡rios conforme necessÃ¡rio
â€¢ Integrar com sistema de observabilidade
"""
        
        return report
    
    async def run_full_real_pipeline(self) -> List[RealTestResults]:
        """Executa pipeline completo com sistema real"""
        
        self.logger.info("ğŸš€ INICIANDO PIPELINE COMPLETO DE TESTES REAIS")
        self.logger.info("="*60)
        self.logger.info("ğŸ¯ Testando sistema agÃªntico real com dados Mock")
        
        results = []
        
        for i, scenario in enumerate(self.test_scenarios, 1):
            self.logger.info(f"\nğŸ“‹ CenÃ¡rio {i}/{len(self.test_scenarios)}: {scenario.name}")
            try:
                result = await self.run_real_scenario(scenario)
                results.append(result)
                self.logger.info(f"   âœ… {scenario.name}: {result.overall_grade}")
                
                # Log do resultado
                log_agent_action(
                    agent_name="test_pipeline",
                    action="scenario_completed",
                    details={
                        "scenario": scenario.name,
                        "grade": result.overall_grade,
                        "success_rate": result.stress_test_results.get('execution_stats', {}).get('success_rate', 0),
                        "system_health": result.system_health.get('overall_status', 'unknown')
                    }
                )
                
            except Exception as e:
                self.logger.error(f"   âŒ Erro no cenÃ¡rio {scenario.name}: {str(e)}")
                log_error(e, context={"scenario": scenario.name, "pipeline": "real_test"})
                continue
        
        # Gerar relatÃ³rio final
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“Š GERANDO RELATÃ“RIO FINAL DO SISTEMA REAL...")
        final_report = self.generate_real_pipeline_report(results)
        print(final_report)
        
        # Salvar resultados
        self._save_test_results(results)
        
        return results
    
    def _save_test_results(self, results: List[RealTestResults]):
        """Salva resultados dos testes em arquivo"""
        try:
            results_data = []
            for result in results:
                results_data.append({
                    "scenario_name": result.scenario_name,
                    "overall_grade": result.overall_grade,
                    "integration_metrics": result.integration_metrics,
                    "system_health": result.system_health,
                    "recommendations": result.recommendations,
                    "timestamp": result.timestamp.isoformat(),
                    "stress_test_summary": {
                        "success_rate": result.stress_test_results.get('execution_stats', {}).get('success_rate', 0),
                        "average_response_time": result.stress_test_results.get('execution_stats', {}).get('average_response_time', 0),
                        "total_questions": result.stress_test_results.get('execution_stats', {}).get('total_questions', 0)
                    }
                })
            
            # Salvar em arquivo JSON
            output_file = Path("hooks_tests") / "real_test_results.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ğŸ’¾ Resultados salvos em: {output_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao salvar resultados: {e}")

async def main():
    """Executa pipeline completo de testes reais"""
    
    print("ğŸ  PIPELINE DE TESTES REAL - REAL ESTATE ASSISTANT")
    print("="*60)
    print("ğŸ¯ Sistema agÃªntico real + OpenRouter/Ollama + Mock Data")
    
    pipeline = RealEstateTestPipeline()
    results = await pipeline.run_full_real_pipeline()
    
    print(f"\nâœ… PIPELINE DE TESTES REAL CONCLUÃDO!")
    print(f"ğŸ“Š {len(results)} cenÃ¡rios executados")
    print(f"ğŸ’¾ Resultados salvos em hooks_tests/real_test_results.json")
    print(f"ğŸ”„ Logs detalhados disponÃ­veis em logs/")

if __name__ == "__main__":
    # Configurar event loop para Windows
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(main()) 