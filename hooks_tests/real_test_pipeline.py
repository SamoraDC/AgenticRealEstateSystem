#!/usr/bin/env python3
"""
Pipeline de Testes REAL - Real Estate Assistant
Pipeline completo integrado com sistema agêntico real usando dados Mock
"""

import asyncio
import json
import time
import requests
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from pathlib import Path
import sys

# Adicionar diretório raiz ao path
sys.path.append(str(Path(__file__).parent.parent))

# Importar componentes reais
from real_stress_testing import RealSystemStressTester, RealVirtualUser
from real_conversation_hooks import (
    RealConversationAnalyzer, ProductionConversationMonitor,
    RealConversationEvent, RealAgentType, RealConversationPhase
)
from app.orchestration.swarm import SwarmOrchestrator
from app.utils.logging import get_logger, log_agent_action, log_api_call, log_error
from config.settings import get_settings

@dataclass
class RealTestScenario:
    """Cenário de teste para sistema real"""
    name: str
    description: str
    virtual_users: List[RealVirtualUser]
    conversation_scripts: List[List[Dict[str, Any]]]
    expected_outcomes: Dict[str, Any]
    performance_thresholds: Dict[str, float]
    mock_data_requirements: List[str]

@dataclass
class RealTestResults:
    """Resultados de testes do sistema real"""
    scenario_name: str
    stress_test_results: Dict[str, Any]
    conversation_analysis: Dict[str, Any]
    integration_metrics: Dict[str, Any]
    mock_data_validation: Dict[str, Any]
    system_health: Dict[str, Any]
    overall_grade: str
    recommendations: List[str]
    timestamp: datetime

class RealEstateTestPipeline:
    """Pipeline integrado de testes para sistema agêntico real"""
    
    def __init__(self):
        self.logger = get_logger("real_test_pipeline")
        self.stress_tester = RealSystemStressTester()
        self.conversation_monitor = ProductionConversationMonitor()
        self.swarm = SwarmOrchestrator()
        self.test_scenarios = self._create_real_test_scenarios()
        self.results_history: List[RealTestResults] = []
        self.mock_api_base = "http://localhost:8000"
        
        # Verificar sistema
        self._verify_system_components()
    
    def _verify_system_components(self):
        """Verifica componentes do sistema real"""
        try:
            # Verificar configurações
            settings = get_settings()
            self.logger.info(f"✅ Configurações carregadas")
            
            # Verificar OpenRouter
            if settings.apis.openrouter_key and settings.apis.openrouter_key != "your_openrouter_api_key_here":
                self.logger.info(f"✅ OpenRouter configurado")
            else:
                self.logger.warning(f"⚠️ OpenRouter não configurado")
            
            # Verificar sistema Mock
            response = requests.get(f"{self.mock_api_base}/api/properties/search", timeout=5)
            if response.status_code == 200:
                data = response.json()
                properties_count = len(data.get("properties", []))
                self.logger.info(f"✅ Sistema Mock disponível - {properties_count} propriedades")
            else:
                self.logger.warning(f"⚠️ Sistema Mock indisponível")
                
        except Exception as e:
            self.logger.error(f"❌ Erro na verificação do sistema: {e}")
    
    def _create_real_test_scenarios(self) -> List[RealTestScenario]:
        """Cria cenários de teste para sistema real"""
        
        scenarios = [
            RealTestScenario(
                name="real_basic_user_journey",
                description="Jornada básica com sistema agêntico real e dados Mock",
                virtual_users=self.stress_tester.virtual_users[:2],
                conversation_scripts=[
                    [
                        {"user_input": "Hi, I'm looking for an apartment", "agent_type": "search_agent", "phase": "greeting"},
                        {"user_input": "Tell me about properties in Miami", "agent_type": "search_agent", "phase": "search_criteria"},
                        {"user_input": "What about the property at 501 Sw 6th Ct?", "agent_type": "property_agent", "phase": "property_details"},
                        {"user_input": "Can I schedule a viewing?", "agent_type": "scheduling_agent", "phase": "scheduling"}
                    ]
                ],
                expected_outcomes={
                    "min_interactions": 4,
                    "agents_used": 3,
                    "phases_covered": 4,
                    "mock_data_accessed": True
                },
                performance_thresholds={
                    "max_response_time": 8.0,  # Mais tolerante para sistema real
                    "min_success_rate": 85.0
                },
                mock_data_requirements=["properties", "addresses", "pricing"]
            ),
            
            RealTestScenario(
                name="real_mock_data_integration",
                description="Teste específico de integração com dados Mock",
                virtual_users=self.stress_tester.virtual_users[2:4],
                conversation_scripts=[
                    [
                        {"user_input": "Show me apartments under $2000", "agent_type": "search_agent", "phase": "search_criteria"},
                        {"user_input": "What properties do you have in Miami-Dade?", "agent_type": "search_agent", "phase": "search_criteria"},
                        {"user_input": "Tell me about 1-bedroom apartments", "agent_type": "property_agent", "phase": "property_details"},
                        {"user_input": "What's the square footage of available units?", "agent_type": "property_agent", "phase": "property_details"},
                        {"user_input": "Are there any properties built after 1920?", "agent_type": "search_agent", "phase": "search_criteria"},
                        {"user_input": "I want to schedule viewings for the best options", "agent_type": "scheduling_agent", "phase": "scheduling"}
                    ]
                ],
                expected_outcomes={
                    "min_interactions": 6,
                    "agents_used": 3,
                    "mock_data_queries": 5,
                    "specific_property_mentions": True
                },
                performance_thresholds={
                    "max_response_time": 10.0,
                    "min_success_rate": 80.0
                },
                mock_data_requirements=["properties", "search_filters", "property_details"]
            ),
            
            RealTestScenario(
                name="real_high_volume_concurrent",
                description="Teste de alto volume com sistema real",
                virtual_users=self.stress_tester.virtual_users,
                conversation_scripts=[],  # Gerado dinamicamente
                expected_outcomes={
                    "concurrent_users": 5,
                    "total_interactions": 25,
                    "system_stability": True
                },
                performance_thresholds={
                    "max_response_time": 12.0,  # Mais tolerante para alta carga
                    "min_success_rate": 75.0
                },
                mock_data_requirements=["properties", "concurrent_access"]
            ),
            
            RealTestScenario(
                name="real_error_handling_resilience",
                description="Teste de resiliência e tratamento de erros do sistema real",
                virtual_users=self.stress_tester.virtual_users[:1],
                conversation_scripts=[
                    [
                        {"user_input": "", "agent_type": "search_agent", "phase": "error_handling"},  # Input vazio
                        {"user_input": "I want a 50-bedroom mansion for $100", "agent_type": "search_agent", "phase": "search_criteria"},  # Critério impossível
                        {"user_input": "What's the weather like today?", "agent_type": "search_agent", "phase": "error_handling"},  # Fora do escopo
                        {"user_input": "Show me property ID 99999999", "agent_type": "property_agent", "phase": "property_details"},  # ID inexistente
                        {"user_input": "Schedule viewing for yesterday", "agent_type": "scheduling_agent", "phase": "error_handling"}  # Data inválida
                    ]
                ],
                expected_outcomes={
                    "error_handling": True,
                    "graceful_degradation": True,
                    "fallback_responses": True
                },
                performance_thresholds={
                    "max_response_time": 15.0,
                    "min_success_rate": 60.0  # Menor devido aos casos de erro
                },
                mock_data_requirements=["error_responses", "fallback_data"]
            )
        ]
        
        return scenarios
    
    async def validate_mock_data_integration(self, scenario: RealTestScenario) -> Dict[str, Any]:
        """Valida integração com dados Mock"""
        validation_results = {
            "mock_api_accessible": False,
            "properties_available": 0,
            "data_quality": "unknown",
            "required_data_present": False,
            "api_response_time": 0.0
        }
        
        try:
            # Testar acesso à API Mock
            start_time = time.time()
            response = requests.get(f"{self.mock_api_base}/api/properties/search", timeout=10)
            api_time = time.time() - start_time
            
            validation_results["api_response_time"] = api_time
            
            if response.status_code == 200:
                validation_results["mock_api_accessible"] = True
                data = response.json()
                properties = data.get("properties", [])
                validation_results["properties_available"] = len(properties)
                
                # Verificar qualidade dos dados
                if properties:
                    sample_property = properties[0]
                    required_fields = ["formattedAddress", "price", "bedrooms", "bathrooms", "squareFootage"]
                    present_fields = sum(1 for field in required_fields if field in sample_property)
                    
                    if present_fields >= 4:
                        validation_results["data_quality"] = "good"
                    elif present_fields >= 2:
                        validation_results["data_quality"] = "fair"
                    else:
                        validation_results["data_quality"] = "poor"
                
                # Verificar se dados necessários estão presentes
                validation_results["required_data_present"] = len(properties) > 0
                
                self.logger.info(f"✅ Mock data validation successful - {len(properties)} properties")
            else:
                self.logger.warning(f"⚠️ Mock API returned status {response.status_code}")
                
        except Exception as e:
            self.logger.error(f"❌ Mock data validation failed: {e}")
            validation_results["error"] = str(e)
        
        return validation_results
    
    async def run_real_scenario(self, scenario: RealTestScenario) -> RealTestResults:
        """Executa cenário de teste com sistema real"""
        
        self.logger.info(f"🎯 Executando cenário real: {scenario.name}")
        self.logger.info(f"📝 {scenario.description}")
        
        start_time = time.time()
        
        # 1. Validar integração com dados Mock
        self.logger.info("   📊 Validando dados Mock...")
        mock_validation = await self.validate_mock_data_integration(scenario)
        
        # 2. Iniciar monitoramento de conversas
        self.conversation_monitor.start_monitoring()
        
        # 3. Executar stress test com sistema real
        if scenario.name == "real_high_volume_concurrent":
            self.logger.info("   🚀 Executando stress test de alto volume...")
            stress_results = await self.stress_tester.run_real_stress_test(
                concurrent_users=len(scenario.virtual_users),
                questions_per_user=5
            )
        else:
            self.logger.info("   🔄 Executando stress test controlado...")
            stress_results = await self.stress_tester.run_real_stress_test(
                concurrent_users=len(scenario.virtual_users),
                questions_per_user=3
            )
        
        # 4. Executar scripts de conversa específicos
        if scenario.conversation_scripts:
            self.logger.info("   💬 Executando scripts de conversa...")
            for i, script in enumerate(scenario.conversation_scripts):
                session_id = f"{scenario.name}_script_{i}"
                await self._execute_conversation_script(session_id, script)
        
        # 5. Parar monitoramento e analisar
        self.conversation_monitor.stop_monitoring()
        conversation_analysis = self.conversation_monitor.get_live_statistics()
        
        # 6. Calcular métricas de integração
        integration_metrics = self._calculate_real_integration_metrics(
            scenario, stress_results, conversation_analysis, mock_validation
        )
        
        # 7. Avaliar saúde do sistema
        system_health = self._assess_real_system_health(
            stress_results, conversation_analysis, mock_validation
        )
        
        # 8. Determinar nota geral
        overall_grade = self._calculate_real_overall_grade(
            scenario, stress_results, conversation_analysis, integration_metrics, system_health
        )
        
        # 9. Gerar recomendações
        recommendations = self._generate_real_recommendations(
            scenario, stress_results, conversation_analysis, integration_metrics, system_health
        )
        
        execution_time = time.time() - start_time
        self.logger.info(f"   ✅ Cenário concluído em {execution_time:.2f}s")
        
        # Criar resultado
        result = RealTestResults(
            scenario_name=scenario.name,
            stress_test_results=stress_results,
            conversation_analysis=conversation_analysis,
            integration_metrics=integration_metrics,
            mock_data_validation=mock_validation,
            system_health=system_health,
            overall_grade=overall_grade,
            recommendations=recommendations,
            timestamp=datetime.now()
        )
        
        self.results_history.append(result)
        return result
    
    async def _execute_conversation_script(self, session_id: str, script: List[Dict[str, Any]]):
        """Executa script de conversa usando sistema real"""
        
        for i, step in enumerate(script):
            try:
                user_input = step["user_input"]
                expected_agent = step.get("agent_type", "search_agent")
                
                if not user_input.strip():  # Input vazio para teste de erro
                    user_input = "Hello"
                
                # Executar através do SwarmOrchestrator real
                message_data = {
                    "content": user_input,
                    "session_id": session_id,
                    "context": {
                        "data_mode": "mock",
                        "property_context": {
                            "formattedAddress": "501 Sw 6th Ct, Apt 215, Miami, FL 33130",
                            "price": 1600,
                            "bedrooms": 1,
                            "bathrooms": 1,
                            "squareFootage": 600
                        }
                    }
                }
                
                start_time = time.time()
                response = await self.swarm.process_message(message_data)
                response_time = time.time() - start_time
                
                # Extrair resposta
                if "messages" in response and response["messages"]:
                    agent_response = response["messages"][-1].content
                    success = True
                else:
                    agent_response = "No response received"
                    success = False
                
                # Monitorar através do sistema de hooks
                await self.conversation_monitor.monitor_conversation(
                    session_id=session_id,
                    user_input=user_input,
                    agent_response=agent_response,
                    agent_type=expected_agent,
                    response_time=response_time,
                    success=success,
                    context={"script_step": i, "scenario": "test_script"}
                )
                
                # Delay entre interações
                await asyncio.sleep(0.5)
                
            except Exception as e:
                self.logger.error(f"Erro no script step {i}: {e}")
                
                # Monitorar erro
                await self.conversation_monitor.monitor_conversation(
                    session_id=session_id,
                    user_input=step.get("user_input", ""),
                    agent_response=f"Error: {str(e)}",
                    agent_type=step.get("agent_type", "search_agent"),
                    response_time=0.0,
                    success=False,
                    context={"error": str(e), "script_step": i}
                )
    
    def _calculate_real_integration_metrics(self, scenario: RealTestScenario, 
                                          stress_results: Dict, conversation_analysis: Dict,
                                          mock_validation: Dict) -> Dict[str, Any]:
        """Calcula métricas de integração para sistema real"""
        
        metrics = {
            "system_coherence": 0.0,
            "agent_coordination": 0.0,
            "response_consistency": 0.0,
            "error_resilience": 0.0,
            "mock_data_integration": 0.0,
            "user_experience_score": 0.0
        }
        
        # Coerência do sistema
        if stress_results and "execution_stats" in stress_results:
            success_rate = stress_results["execution_stats"]["success_rate"]
            metrics["system_coherence"] = min(success_rate / 100.0, 1.0)
        
        # Coordenação entre agentes
        if conversation_analysis and "agent_transition_patterns" in conversation_analysis:
            transitions = conversation_analysis["agent_transition_patterns"]
            total_transitions = transitions.get("total_transitions", 0)
            unique_patterns = transitions.get("unique_patterns", 0)
            if total_transitions > 0:
                metrics["agent_coordination"] = min(unique_patterns / 3.0, 1.0)  # Esperamos pelo menos 3 padrões
        
        # Consistência de resposta
        if stress_results and "execution_stats" in stress_results:
            avg_time = stress_results["execution_stats"]["average_response_time"]
            threshold = scenario.performance_thresholds.get("max_response_time", 8.0)
            metrics["response_consistency"] = max(0.0, 1.0 - (avg_time / threshold))
        
        # Resiliência a erros
        if scenario.name == "real_error_handling_resilience":
            # Para cenários de erro, avaliar se o sistema se manteve estável
            metrics["error_resilience"] = 0.8 if metrics["system_coherence"] > 0.5 else 0.3
        else:
            metrics["error_resilience"] = metrics["system_coherence"]
        
        # Integração com dados Mock
        if mock_validation.get("mock_api_accessible", False):
            mock_score = 0.0
            if mock_validation.get("properties_available", 0) > 0:
                mock_score += 0.4
            if mock_validation.get("data_quality") == "good":
                mock_score += 0.4
            elif mock_validation.get("data_quality") == "fair":
                mock_score += 0.2
            if mock_validation.get("api_response_time", 10) < 5.0:
                mock_score += 0.2
            metrics["mock_data_integration"] = mock_score
        
        # Score de experiência do usuário (média ponderada)
        weights = [0.25, 0.2, 0.2, 0.15, 0.2]  # Pesos para cada métrica
        values = [
            metrics["system_coherence"],
            metrics["agent_coordination"], 
            metrics["response_consistency"],
            metrics["error_resilience"],
            metrics["mock_data_integration"]
        ]
        metrics["user_experience_score"] = sum(w * v for w, v in zip(weights, values))
        
        return metrics
    
    def _assess_real_system_health(self, stress_results: Dict, conversation_analysis: Dict,
                                 mock_validation: Dict) -> Dict[str, Any]:
        """Avalia saúde do sistema real"""
        
        health = {
            "overall_status": "unknown",
            "components": {
                "stress_testing": "unknown",
                "conversation_flow": "unknown", 
                "mock_data": "unknown",
                "agent_coordination": "unknown"
            },
            "metrics": {},
            "issues": []
        }
        
        # Avaliar stress testing
        if stress_results and "execution_stats" in stress_results:
            success_rate = stress_results["execution_stats"]["success_rate"]
            if success_rate >= 90:
                health["components"]["stress_testing"] = "healthy"
            elif success_rate >= 70:
                health["components"]["stress_testing"] = "warning"
            else:
                health["components"]["stress_testing"] = "critical"
                health["issues"].append(f"Low stress test success rate: {success_rate:.1f}%")
        
        # Avaliar fluxo de conversa
        if conversation_analysis and "overall_success_rate" in conversation_analysis:
            conv_success = conversation_analysis["overall_success_rate"]
            if conv_success >= 85:
                health["components"]["conversation_flow"] = "healthy"
            elif conv_success >= 65:
                health["components"]["conversation_flow"] = "warning"
            else:
                health["components"]["conversation_flow"] = "critical"
                health["issues"].append(f"Poor conversation success rate: {conv_success:.1f}%")
        
        # Avaliar dados Mock
        if mock_validation.get("mock_api_accessible", False):
            if mock_validation.get("data_quality") == "good":
                health["components"]["mock_data"] = "healthy"
            elif mock_validation.get("data_quality") == "fair":
                health["components"]["mock_data"] = "warning"
            else:
                health["components"]["mock_data"] = "critical"
        else:
            health["components"]["mock_data"] = "critical"
            health["issues"].append("Mock API not accessible")
        
        # Avaliar coordenação de agentes
        if conversation_analysis and "agent_transition_patterns" in conversation_analysis:
            transitions = conversation_analysis["agent_transition_patterns"]
            if transitions.get("total_transitions", 0) > 0:
                health["components"]["agent_coordination"] = "healthy"
            else:
                health["components"]["agent_coordination"] = "warning"
                health["issues"].append("No agent transitions detected")
        
        # Determinar status geral
        component_scores = {
            "healthy": 1.0,
            "warning": 0.6,
            "critical": 0.2,
            "unknown": 0.0
        }
        
        total_score = sum(component_scores.get(status, 0) for status in health["components"].values())
        avg_score = total_score / len(health["components"])
        
        if avg_score >= 0.8:
            health["overall_status"] = "healthy"
        elif avg_score >= 0.6:
            health["overall_status"] = "warning"
        else:
            health["overall_status"] = "critical"
        
        health["metrics"]["component_average"] = avg_score
        health["metrics"]["total_issues"] = len(health["issues"])
        
        return health
    
    def _calculate_real_overall_grade(self, scenario: RealTestScenario, stress_results: Dict,
                                    conversation_analysis: Dict, integration_metrics: Dict,
                                    system_health: Dict) -> str:
        """Calcula nota geral para sistema real"""
        
        # Score de experiência do usuário
        ux_score = integration_metrics.get("user_experience_score", 0.0)
        
        # Score de saúde do sistema
        health_score = system_health["metrics"].get("component_average", 0.0)
        
        # Verificar compliance com thresholds
        threshold_compliance = 0.0
        if stress_results and "execution_stats" in stress_results:
            success_rate = stress_results["execution_stats"]["success_rate"]
            avg_response_time = stress_results["execution_stats"]["average_response_time"]
            
            min_success = scenario.performance_thresholds.get("min_success_rate", 75.0)
            max_time = scenario.performance_thresholds.get("max_response_time", 8.0)
            
            success_compliance = min(success_rate / min_success, 1.0)
            time_compliance = min(max_time / avg_response_time, 1.0)
            
            threshold_compliance = (success_compliance + time_compliance) / 2.0
        
        # Calcular nota final (mais tolerante para sistema real)
        final_score = (ux_score * 0.4) + (health_score * 0.3) + (threshold_compliance * 0.3)
        
        if final_score >= 0.85:
            return "A+ (Excelente para Sistema Real)"
        elif final_score >= 0.75:
            return "A (Muito Bom para Sistema Real)"
        elif final_score >= 0.65:
            return "B (Bom para Sistema Real)"
        elif final_score >= 0.55:
            return "C (Satisfatório para Sistema Real)"
        else:
            return "D (Precisa Melhorar)"
    
    def _generate_real_recommendations(self, scenario: RealTestScenario, stress_results: Dict,
                                     conversation_analysis: Dict, integration_metrics: Dict,
                                     system_health: Dict) -> List[str]:
        """Gera recomendações para sistema real"""
        
        recommendations = []
        
        # Recomendações baseadas em saúde do sistema
        if system_health["overall_status"] == "critical":
            recommendations.append("🚨 CRÍTICO: Revisar configuração do sistema imediatamente")
        elif system_health["overall_status"] == "warning":
            recommendations.append("⚠️ Monitorar sistema de perto - possíveis problemas")
        
        # Recomendações específicas por componente
        for component, status in system_health["components"].items():
            if status == "critical":
                if component == "mock_data":
                    recommendations.append("🔧 Verificar conectividade com API Mock")
                elif component == "stress_testing":
                    recommendations.append("⚡ Otimizar performance do sistema agêntico")
                elif component == "conversation_flow":
                    recommendations.append("💬 Melhorar fluxo de conversa entre agentes")
                elif component == "agent_coordination":
                    recommendations.append("🤝 Implementar melhor coordenação entre agentes")
        
        # Recomendações baseadas em métricas
        mock_score = integration_metrics.get("mock_data_integration", 0.0)
        if mock_score < 0.7:
            recommendations.append("📊 Melhorar integração com dados Mock")
        
        ux_score = integration_metrics.get("user_experience_score", 0.0)
        if ux_score < 0.6:
            recommendations.append("👤 Focar em melhorar experiência do usuário")
        
        # Recomendações baseadas em performance
        if stress_results and "execution_stats" in stress_results:
            avg_time = stress_results["execution_stats"]["average_response_time"]
            if avg_time > 8.0:
                recommendations.append("⏱️ Otimizar tempo de resposta dos agentes")
            
            success_rate = stress_results["execution_stats"]["success_rate"]
            if success_rate < 80.0:
                recommendations.append("🎯 Melhorar taxa de sucesso geral")
        
        # Recomendações específicas do cenário
        if scenario.name == "real_error_handling_resilience":
            recommendations.append("🛡️ Implementar tratamento de erro mais robusto")
        elif scenario.name == "real_high_volume_concurrent":
            recommendations.append("📈 Considerar otimizações para alta carga")
        
        if not recommendations:
            recommendations.append("✅ Sistema funcionando bem - manter monitoramento")
        
        return recommendations
    
    def generate_real_pipeline_report(self, results: List[RealTestResults]) -> str:
        """Gera relatório consolidado do pipeline real"""
        
        if not results:
            return "❌ Nenhum resultado disponível para relatório"
        
        report = f"""
🏠 RELATÓRIO DO PIPELINE DE TESTES REAL - REAL ESTATE ASSISTANT
{'='*75}

🎯 RESUMO EXECUTIVO:
• Total de Cenários Testados: {len(results)}
• Sistema: Agêntico Real (OpenRouter + Ollama + Mock Data)
• Data da Execução: {results[-1].timestamp.strftime('%Y-%m-%d %H:%M:%S')}
• Duração Total: {sum((r.timestamp - results[0].timestamp).total_seconds() for r in results[1:]):.1f}s

📊 RESULTADOS POR CENÁRIO:
"""
        
        for result in results:
            report += f"""
📋 {result.scenario_name.upper().replace('_', ' ')}:
   • Nota Geral: {result.overall_grade}
   • Score UX: {result.integration_metrics.get('user_experience_score', 0):.2f}
   • Saúde do Sistema: {result.system_health.get('overall_status', 'unknown').upper()}
   • Taxa de Sucesso: {result.stress_test_results.get('execution_stats', {}).get('success_rate', 0):.1f}%
   • Tempo Médio: {result.stress_test_results.get('execution_stats', {}).get('average_response_time', 0):.2f}s
   • Mock Data Score: {result.integration_metrics.get('mock_data_integration', 0):.2f}
"""
        
        # Calcular métricas agregadas
        avg_ux_score = sum(r.integration_metrics.get('user_experience_score', 0) for r in results) / len(results)
        avg_success_rate = sum(r.stress_test_results.get('execution_stats', {}).get('success_rate', 0) for r in results) / len(results)
        avg_mock_score = sum(r.integration_metrics.get('mock_data_integration', 0) for r in results) / len(results)
        
        # Contar status de saúde
        health_counts = {}
        for result in results:
            status = result.system_health.get('overall_status', 'unknown')
            health_counts[status] = health_counts.get(status, 0) + 1
        
        report += f"""
📈 MÉTRICAS AGREGADAS DO SISTEMA REAL:
• Score UX Médio: {avg_ux_score:.2f}
• Taxa de Sucesso Média: {avg_success_rate:.1f}%
• Score Mock Data Médio: {avg_mock_score:.2f}
• Cenários com Nota A+/A: {sum(1 for r in results if r.overall_grade.startswith('A'))}/{len(results)}

🏥 SAÚDE DO SISTEMA:
"""
        for status, count in health_counts.items():
            report += f"• {status.upper()}: {count} cenários\n"
        
        report += f"""
🔧 PRINCIPAIS RECOMENDAÇÕES:
"""
        
        # Compilar todas as recomendações
        all_recommendations = []
        for result in results:
            all_recommendations.extend(result.recommendations)
        
        # Contar recomendações mais comuns
        rec_counts = {}
        for rec in all_recommendations:
            rec_counts[rec] = rec_counts.get(rec, 0) + 1
        
        # Mostrar top 5 recomendações
        top_recommendations = sorted(rec_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        for rec, count in top_recommendations:
            report += f"• {rec} (mencionado {count}x)\n"
        
        # Determinar status geral do sistema real
        healthy_count = health_counts.get('healthy', 0)
        warning_count = health_counts.get('warning', 0)
        critical_count = health_counts.get('critical', 0)
        
        if critical_count > 0:
            status = "🔴 SISTEMA REAL PRECISA DE ATENÇÃO CRÍTICA"
        elif warning_count > healthy_count:
            status = "🟡 SISTEMA REAL ESTÁVEL - MONITORAMENTO RECOMENDADO"
        elif avg_success_rate >= 80 and avg_ux_score >= 0.7:
            status = "🟢 SISTEMA REAL EM EXCELENTE ESTADO"
        else:
            status = "🟡 SISTEMA REAL FUNCIONANDO - MELHORIAS RECOMENDADAS"
        
        report += f"\n{status}\n"
        
        # Informações específicas do sistema real
        report += f"""
🔗 INTEGRAÇÃO COM SISTEMA REAL:
• OpenRouter/Ollama: Integrado e testado
• Dados Mock: Validados e acessíveis
• Hooks de Conversa: Ativos e monitorando
• SwarmOrchestrator: Funcionando com agentes reais
• Logs: Disponíveis em logs/

📊 PRÓXIMOS PASSOS PARA PRODUÇÃO:
• Monitorar métricas continuamente
• Executar testes semanais
• Configurar alertas automáticos
• Expandir cenários conforme necessário
• Integrar com sistema de observabilidade
"""
        
        return report
    
    async def run_full_real_pipeline(self) -> List[RealTestResults]:
        """Executa pipeline completo com sistema real"""
        
        self.logger.info("🚀 INICIANDO PIPELINE COMPLETO DE TESTES REAIS")
        self.logger.info("="*60)
        self.logger.info("🎯 Testando sistema agêntico real com dados Mock")
        
        results = []
        
        for i, scenario in enumerate(self.test_scenarios, 1):
            self.logger.info(f"\n📋 Cenário {i}/{len(self.test_scenarios)}: {scenario.name}")
            try:
                result = await self.run_real_scenario(scenario)
                results.append(result)
                self.logger.info(f"   ✅ {scenario.name}: {result.overall_grade}")
                
                # Log do resultado
                log_agent_action(
                    agent_name="test_pipeline",
                    action="scenario_completed",
                    details={
                        "scenario": scenario.name,
                        "grade": result.overall_grade,
                        "success_rate": result.stress_test_results.get('execution_stats', {}).get('success_rate', 0),
                        "system_health": result.system_health.get('overall_status', 'unknown')
                    }
                )
                
            except Exception as e:
                self.logger.error(f"   ❌ Erro no cenário {scenario.name}: {str(e)}")
                log_error(e, context={"scenario": scenario.name, "pipeline": "real_test"})
                continue
        
        # Gerar relatório final
        self.logger.info("\n" + "="*60)
        self.logger.info("📊 GERANDO RELATÓRIO FINAL DO SISTEMA REAL...")
        final_report = self.generate_real_pipeline_report(results)
        print(final_report)
        
        # Salvar resultados
        self._save_test_results(results)
        
        return results
    
    def _save_test_results(self, results: List[RealTestResults]):
        """Salva resultados dos testes em arquivo"""
        try:
            results_data = []
            for result in results:
                results_data.append({
                    "scenario_name": result.scenario_name,
                    "overall_grade": result.overall_grade,
                    "integration_metrics": result.integration_metrics,
                    "system_health": result.system_health,
                    "recommendations": result.recommendations,
                    "timestamp": result.timestamp.isoformat(),
                    "stress_test_summary": {
                        "success_rate": result.stress_test_results.get('execution_stats', {}).get('success_rate', 0),
                        "average_response_time": result.stress_test_results.get('execution_stats', {}).get('average_response_time', 0),
                        "total_questions": result.stress_test_results.get('execution_stats', {}).get('total_questions', 0)
                    }
                })
            
            # Salvar em arquivo JSON
            output_file = Path("hooks_tests") / "real_test_results.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"💾 Resultados salvos em: {output_file}")
            
        except Exception as e:
            self.logger.error(f"❌ Erro ao salvar resultados: {e}")

async def main():
    """Executa pipeline completo de testes reais"""
    
    print("🏠 PIPELINE DE TESTES REAL - REAL ESTATE ASSISTANT")
    print("="*60)
    print("🎯 Sistema agêntico real + OpenRouter/Ollama + Mock Data")
    
    pipeline = RealEstateTestPipeline()
    results = await pipeline.run_full_real_pipeline()
    
    print(f"\n✅ PIPELINE DE TESTES REAL CONCLUÍDO!")
    print(f"📊 {len(results)} cenários executados")
    print(f"💾 Resultados salvos em hooks_tests/real_test_results.json")
    print(f"🔄 Logs detalhados disponíveis em logs/")

if __name__ == "__main__":
    # Configurar event loop para Windows
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(main()) 