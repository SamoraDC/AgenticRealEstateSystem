#!/usr/bin/env python3
"""
DEMONSTRAÃ‡ÃƒO FINAL: llama-3.1-8b-instant (Groq) vs Google Gemini para NLL

RESULTADO ESPERADO:
- llama-3.1-8b-instant: âœ… Texto bÃ¡sico, âŒ Sem logprobs/NLL
- Google Gemini: âœ… Texto + âœ… NLL via sampling

DEMONSTRAÃ‡ÃƒO: ComparaÃ§Ã£o lado a lado das capacidades
"""

import os
import asyncio
from typing import Dict, Any
from dotenv import load_dotenv

load_dotenv()

class LlamaVsGeminiComparator:
    """Comparador final: llama-3.1-8b-instant vs Google Gemini"""
    
    def __init__(self):
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.groq_api_key = os.getenv("GROQ_API_KEY")
        
        # Texto de teste
        self.test_text = "Artificial intelligence revolutionizes how we process information and make decisions."
        
        self.setup_clients()
    
    def setup_clients(self):
        """Configurar clientes"""
        
        # Google Gemini
        if self.google_api_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.google_api_key)
                self.google_model = genai.GenerativeModel('gemini-2.0-flash-lite')
                print("âœ… Google Gemini configurado")
            except ImportError:
                print("âŒ Google Generative AI nÃ£o disponÃ­vel")
                self.google_model = None
        else:
            self.google_model = None
        
        # Groq llama-3.1-8b-instant
        if self.groq_api_key:
            try:
                from groq import Groq
                self.groq_client = Groq(api_key=self.groq_api_key)
                print("âœ… Groq llama-3.1-8b-instant configurado")
            except ImportError:
                print("âŒ Groq nÃ£o disponÃ­vel")
                self.groq_client = None
        else:
            self.groq_client = None
    
    def print_section(self, title: str):
        """Imprimir seÃ§Ã£o formatada"""
        print(f"\n{'='*70}")
        print(f" {title}")
        print(f"{'='*70}")
    
    async def test_llama_instant(self) -> Dict[str, Any]:
        """Testar llama-3.1-8b-instant"""
        
        print("ğŸŸ¢ TESTANDO: llama-3.1-8b-instant (Groq)")
        print("-" * 50)
        
        if not self.groq_client:
            return {"error": "Groq nÃ£o disponÃ­vel"}
        
        result = {
            "model": "llama-3.1-8b-instant",
            "provider": "Groq",
            "basic_functionality": False,
            "logprobs_available": False,
            "nll_calculable": False
        }
        
        # Teste 1: Funcionalidade bÃ¡sica
        print(f"ğŸ“ Texto: {self.test_text}")
        print("ğŸ” Testando funcionalidade bÃ¡sica...")
        
        try:
            response = self.groq_client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "user", "content": f"Analise este texto: {self.test_text}"}],
                temperature=0.3,
                max_tokens=100
            )
            
            response_text = response.choices[0].message.content
            print(f"âœ… Resposta bÃ¡sica recebida ({len(response_text)} chars)")
            print(f"ğŸ“¤ Resposta: {response_text[:120]}...")
            
            result.update({
                "basic_functionality": True,
                "response_text": response_text,
                "response_length": len(response_text)
            })
            
        except Exception as e:
            print(f"âŒ Erro bÃ¡sico: {e}")
            result["basic_error"] = str(e)
            return result
        
        # Teste 2: Logprobs
        print("\nğŸ” Testando logprobs...")
        
        try:
            logprobs_response = self.groq_client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "user", "content": self.test_text}],
                temperature=0.1,
                max_tokens=50,
                logprobs=True,
                top_logprobs=5
            )
            
            print("âœ… Logprobs suportados! (surpresa)")
            result["logprobs_available"] = True
            # Aqui poderia analisar os logprobs...
            
        except Exception as e:
            print(f"âŒ Logprobs rejeitados: {e}")
            result["logprobs_error"] = str(e)
        
        print("\nğŸ“Š RESULTADO llama-3.1-8b-instant:")
        print(f"   âœ… Funcionalidade bÃ¡sica: {'SIM' if result['basic_functionality'] else 'NÃƒO'}")
        print(f"   ğŸ“Š Logprobs: {'SIM' if result['logprobs_available'] else 'NÃƒO'}")
        print(f"   ğŸ¯ NLL: {'SIM' if result['nll_calculable'] else 'NÃƒO'}")
        
        return result
    
    async def test_google_gemini(self) -> Dict[str, Any]:
        """Testar Google Gemini"""
        
        print("\nğŸ”µ TESTANDO: Google Gemini 2.0 Flash Lite")
        print("-" * 50)
        
        if not self.google_model:
            return {"error": "Google Gemini nÃ£o disponÃ­vel"}
        
        result = {
            "model": "gemini-2.0-flash-lite",
            "provider": "Google",
            "basic_functionality": False,
            "nll_calculable": False
        }
        
        # Teste 1: Funcionalidade bÃ¡sica
        print(f"ğŸ“ Texto: {self.test_text}")
        print("ğŸ” Testando funcionalidade bÃ¡sica...")
        
        try:
            basic_response = self.google_model.generate_content(
                f"Analise este texto: {self.test_text}",
                generation_config={"temperature": 0.3, "max_output_tokens": 100}
            )
            
            response_text = basic_response.text
            print(f"âœ… Resposta bÃ¡sica recebida ({len(response_text)} chars)")
            print(f"ğŸ“¤ Resposta: {response_text[:120]}...")
            
            result.update({
                "basic_functionality": True,
                "response_text": response_text,
                "response_length": len(response_text)
            })
            
        except Exception as e:
            print(f"âŒ Erro bÃ¡sico: {e}")
            result["basic_error"] = str(e)
            return result
        
        # Teste 2: NLL via sampling
        print("\nğŸ” Testando NLL via sampling...")
        
        try:
            # Usar calculadora jÃ¡ implementada
            from nll_calculator_final import calculate_nll
            
            nll_value = await calculate_nll(self.test_text)
            
            print(f"âœ… NLL calculado via sampling: {nll_value:.4f}")
            
            result.update({
                "nll_calculable": True,
                "nll_value": nll_value,
                "nll_method": "sampling_estimation"
            })
            
        except Exception as e:
            print(f"âŒ Erro NLL: {e}")
            result["nll_error"] = str(e)
        
        print("\nğŸ“Š RESULTADO Google Gemini:")
        print(f"   âœ… Funcionalidade bÃ¡sica: {'SIM' if result['basic_functionality'] else 'NÃƒO'}")
        print(f"   ğŸ“Š Logprobs: âš ï¸ Parcial (via sampling)")
        print(f"   ğŸ¯ NLL: {'SIM' if result['nll_calculable'] else 'NÃƒO'}")
        if result.get('nll_calculable'):
            print(f"   ğŸ“ˆ NLL obtido: {result['nll_value']:.4f}")
        
        return result
    
    def generate_final_comparison(self, llama_result: Dict, gemini_result: Dict):
        """Gerar comparaÃ§Ã£o final detalhada"""
        
        self.print_section("COMPARAÃ‡ÃƒO FINAL: llama-3.1-8b-instant vs Google Gemini")
        
        print(f"ğŸ¯ OBJETIVO: Calcular NLL para texto de teste")
        print(f"ğŸ“ Texto analisado: '{self.test_text}'")
        
        # Tabela de comparaÃ§Ã£o
        print(f"\nğŸ“‹ TABELA COMPARATIVA:")
        print("-" * 70)
        print(f"{'Aspecto':<20} {'llama-3.1-8b':<20} {'Google Gemini':<25}")
        print("-" * 70)
        
        # Funcionalidade bÃ¡sica
        llama_basic = "âœ… SIM" if llama_result.get("basic_functionality") else "âŒ NÃƒO"
        gemini_basic = "âœ… SIM" if gemini_result.get("basic_functionality") else "âŒ NÃƒO"
        print(f"{'Funcionalidade':<20} {llama_basic:<20} {gemini_basic:<25}")
        
        # Logprobs
        llama_logprobs = "âœ… SIM" if llama_result.get("logprobs_available") else "âŒ NÃƒO"
        gemini_logprobs = "âš ï¸ Parcial"
        print(f"{'Logprobs':<20} {llama_logprobs:<20} {gemini_logprobs:<25}")
        
        # NLL
        llama_nll = "âœ… SIM" if llama_result.get("nll_calculable") else "âŒ NÃƒO"
        gemini_nll = "âœ… SIM" if gemini_result.get("nll_calculable") else "âŒ NÃƒO"
        print(f"{'NLL CalculÃ¡vel':<20} {llama_nll:<20} {gemini_nll:<25}")
        
        # Valor NLL
        llama_nll_val = f"{llama_result.get('nll_value', 0):.4f}" if llama_result.get("nll_calculable") else "N/A"
        gemini_nll_val = f"{gemini_result.get('nll_value', 0):.4f}" if gemini_result.get("nll_calculable") else "N/A"
        print(f"{'Valor NLL':<20} {llama_nll_val:<20} {gemini_nll_val:<25}")
        
        print("-" * 70)
        
        # AnÃ¡lise detalhada
        print(f"\nğŸ” ANÃLISE DETALHADA:")
        
        print(f"\nğŸŸ¢ llama-3.1-8b-instant (Groq):")
        if llama_result.get("basic_functionality"):
            print(f"   âœ… Gera texto de qualidade")
            print(f"   ğŸ“ Resposta: {len(llama_result.get('response_text', ''))} caracteres")
        
        if not llama_result.get("logprobs_available"):
            error = llama_result.get("logprobs_error", "Erro desconhecido")
            print(f"   âŒ Logprobs: {error}")
            print(f"   âŒ ConsequÃªncia: ImpossÃ­vel calcular NLL diretamente")
        
        print(f"\nğŸ”µ Google Gemini 2.0 Flash Lite:")
        if gemini_result.get("basic_functionality"):
            print(f"   âœ… Gera texto de qualidade")
            print(f"   ğŸ“ Resposta: {len(gemini_result.get('response_text', ''))} caracteres")
        
        if gemini_result.get("nll_calculable"):
            print(f"   âœ… NLL via sampling: {gemini_result['nll_value']:.4f}")
            print(f"   âœ… MÃ©todo: EstimaÃ§Ã£o por amostragem mÃºltipla")
        else:
            print(f"   âŒ NLL falhou: {gemini_result.get('nll_error', 'Erro desconhecido')}")
        
        # ConclusÃ£o e recomendaÃ§Ãµes
        print(f"\nğŸ¯ CONCLUSÃƒO:")
        
        if gemini_result.get("nll_calculable") and not llama_result.get("nll_calculable"):
            print(f"ğŸ† VENCEDOR PARA NLL: Google Gemini")
            print(f"   ğŸ“ˆ NLL obtido: {gemini_result['nll_value']:.4f}")
            print(f"   ğŸ’¡ RecomendaÃ§Ã£o: Use Google Gemini para cÃ¡lculos de NLL")
        
        elif llama_result.get("nll_calculable") and not gemini_result.get("nll_calculable"):
            print(f"ğŸ† VENCEDOR PARA NLL: llama-3.1-8b-instant")
            print(f"   ğŸ“ˆ NLL obtido: {llama_result['nll_value']:.4f}")
            print(f"   ğŸ’¡ RecomendaÃ§Ã£o: Use llama-3.1-8b-instant para NLL")
        
        elif not llama_result.get("nll_calculable") and not gemini_result.get("nll_calculable"):
            print(f"âŒ NENHUM MODELO CALCULOU NLL")
            print(f"   ğŸ’¡ RecomendaÃ§Ã£o: Considere OpenAI ou modelos locais")
        
        else:
            print(f"ğŸ¤ AMBOS CALCULARAM NLL")
            print(f"   ğŸ“Š llama-3.1-8b: {llama_result.get('nll_value', 0):.4f}")
            print(f"   ğŸ“Š Google Gemini: {gemini_result.get('nll_value', 0):.4f}")
        
        print(f"\nğŸ”§ PARA USO GERAL:")
        if llama_result.get("basic_functionality"):
            print(f"   âœ… llama-3.1-8b-instant: Excelente para geraÃ§Ã£o de texto")
        if gemini_result.get("basic_functionality"):
            print(f"   âœ… Google Gemini: Excelente para texto + anÃ¡lises")

async def main():
    """FunÃ§Ã£o principal da demonstraÃ§Ã£o"""
    
    print("ğŸ”¬ DEMONSTRAÃ‡ÃƒO: llama-3.1-8b-instant vs Google Gemini")
    print("=" * 70)
    print("Comparando capacidades para cÃ¡lculo de NLL")
    
    comparator = LlamaVsGeminiComparator()
    
    # Testar ambos os modelos
    llama_result = await comparator.test_llama_instant()
    gemini_result = await comparator.test_google_gemini()
    
    # Gerar comparaÃ§Ã£o final
    comparator.generate_final_comparison(llama_result, gemini_result)
    
    print(f"\nğŸ DemonstraÃ§Ã£o concluÃ­da!")
    print(f"ğŸ’¡ Use o modelo que melhor atende suas necessidades de NLL.")

if __name__ == "__main__":
    asyncio.run(main()) 