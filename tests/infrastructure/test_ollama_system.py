#!/usr/bin/env python3
"""
Teste do Sistema Ollama para Fallback Inteligente
"""

import asyncio
import httpx
from app.utils.ollama_fallback import get_ollama_fallback, generate_intelligent_fallback

async def test_ollama_availability():
    """Testar se Ollama estÃ¡ disponÃ­vel."""
    
    print("ğŸ” Testing Ollama Availability")
    print("=" * 50)
    
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            print("ğŸ“¡ Checking if Ollama is running...")
            response = await client.get("http://localhost:11434/api/tags")
            
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model.get("name", "") for model in models]
                
                print(f"âœ… Ollama is running!")
                print(f"ğŸ“¦ Available models: {len(models)}")
                
                for model in models:
                    name = model.get("name", "Unknown")
                    size = model.get("size", 0)
                    print(f"   â€¢ {name} ({size // 1024 // 1024} MB)")
                
                # Verificar se gemma3n:e2b estÃ¡ disponÃ­vel
                if "gemma3n:e2b" in model_names:
                    print(f"âœ… Target model 'gemma3n:e2b' is available!")
                    return True, True
                else:
                    print(f"âš ï¸ Target model 'gemma3n:e2b' not found")
                    return True, False
                    
            else:
                print(f"âŒ Ollama API error: {response.status_code}")
                return False, False
                
    except Exception as e:
        print(f"âŒ Ollama not available: {e}")
        print("ğŸ’¡ Make sure Ollama is installed and running:")
        print("   â€¢ Install: https://ollama.ai/")
        print("   â€¢ Start: ollama serve")
        return False, False

async def test_model_pull():
    """Testar pull do modelo gemma3n:e2b."""
    
    print("\nğŸ”„ Testing Model Pull")
    print("=" * 50)
    
    try:
        async with httpx.AsyncClient(timeout=300.0) as client:
            print("ğŸ“¥ Attempting to pull model 'gemma3n:e2b'...")
            print("â³ This may take a few minutes for first-time download...")
            
            response = await client.post(
                "http://localhost:11434/api/pull",
                json={"name": "gemma3n:e2b"},
                timeout=300.0
            )
            
            if response.status_code == 200:
                print("âœ… Model pull successful!")
                return True
            else:
                print(f"âŒ Model pull failed: {response.status_code}")
                print(f"Response: {response.text}")
                return False
                
    except Exception as e:
        print(f"âŒ Error during model pull: {e}")
        return False

async def test_ollama_generation():
    """Testar geraÃ§Ã£o de resposta com Ollama."""
    
    print("\nğŸ§ª Testing Ollama Generation")
    print("=" * 50)
    
    try:
        ollama = get_ollama_fallback()
        
        # Teste 1: Search Agent
        print("ğŸ” Testing search agent response...")
        search_response = await ollama.generate_response(
            "search_agent",
            "I'm looking for a 2-bedroom apartment in Miami",
            {},
            "mock"
        )
        print(f"âœ… Search response ({len(search_response)} chars):")
        print(f"ğŸ“ Preview: {search_response[:150]}...")
        
        # Teste 2: Property Agent
        print("\nğŸ  Testing property agent response...")
        property_context = {
            "formattedAddress": "467 Nw 8th St, Apt 3, Miami, FL 33136",
            "price": 1450,
            "bedrooms": 0,
            "bathrooms": 1,
            "squareFootage": 502,
            "propertyType": "Apartment",
            "yearBuilt": 1950
        }
        
        property_response = await ollama.generate_response(
            "property_agent",
            "How much is the rent for this property?",
            property_context,
            "mock"
        )
        print(f"âœ… Property response ({len(property_response)} chars):")
        print(f"ğŸ“ Preview: {property_response[:150]}...")
        
        # Teste 3: Scheduling Agent
        print("\nğŸ“… Testing scheduling agent response...")
        scheduling_response = await ollama.generate_response(
            "scheduling_agent",
            "I'd like to schedule a visit to this property",
            property_context,
            "mock"
        )
        print(f"âœ… Scheduling response ({len(scheduling_response)} chars):")
        print(f"ğŸ“ Preview: {scheduling_response[:150]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ Ollama generation failed: {e}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")
        return False

async def test_fallback_function():
    """Testar funÃ§Ã£o principal de fallback."""
    
    print("\nğŸ¯ Testing Main Fallback Function")
    print("=" * 50)
    
    try:
        response = await generate_intelligent_fallback(
            "property_agent",
            "Tell me about this property",
            {
                "formattedAddress": "Test Property, Miami, FL",
                "price": 2000,
                "bedrooms": 2,
                "bathrooms": 2
            },
            "mock"
        )
        
        print(f"âœ… Fallback function successful ({len(response)} chars):")
        print(f"ğŸ“ Response: {response}")
        return True
        
    except Exception as e:
        print(f"âŒ Fallback function failed: {e}")
        return False

async def main():
    """Executar todos os testes."""
    
    print("ğŸš€ Ollama Fallback System Test")
    print("=" * 70)
    
    # Teste 1: Verificar disponibilidade
    ollama_running, model_available = await test_ollama_availability()
    
    # Teste 2: Pull do modelo se necessÃ¡rio
    if ollama_running and not model_available:
        model_pulled = await test_model_pull()
        if not model_pulled:
            print("\nâŒ Cannot proceed without model. Please install manually:")
            print("   ollama pull gemma3n:e2b")
            return
    elif not ollama_running:
        print("\nâŒ Cannot proceed without Ollama running.")
        return
    
    # Teste 3: GeraÃ§Ã£o de respostas
    generation_success = await test_ollama_generation()
    
    # Teste 4: FunÃ§Ã£o principal
    fallback_success = await test_fallback_function()
    
    # Resumo final
    print("\n" + "=" * 70)
    print("ğŸ“‹ TEST SUMMARY:")
    print(f"   Ollama Running: {'âœ…' if ollama_running else 'âŒ'}")
    print(f"   Model Available: {'âœ…' if model_available else 'âŒ'}")
    print(f"   Generation: {'âœ…' if generation_success else 'âŒ'}")
    print(f"   Fallback Function: {'âœ…' if fallback_success else 'âŒ'}")
    
    if all([ollama_running, generation_success, fallback_success]):
        print("\nğŸ‰ All tests passed! Ollama fallback system is ready!")
        print("ğŸ’¡ The system will now use Ollama when OpenRouter fails.")
    else:
        print("\nâš ï¸ Some tests failed. Please check Ollama installation.")

if __name__ == "__main__":
    asyncio.run(main()) 